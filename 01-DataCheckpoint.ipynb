{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "Team:\n",
    "- Ashley Vo: Conceptualization, Writing – review & editing, Project administration, Data curation\n",
    "- Dorje Pradhan: Conceptualization, Writing – original draft, Writing – review & editing, Data curation\n",
    "- Kilhoon (Andy) Kim: Writing – original draft, Writing – review & editing, Data curation\n",
    "- Kobe Wood: Data curation, Writing – original draft, Writing – review & editing\n",
    "- Vy (Kiet) Dang: Background research, Writing – original draft, Data curation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the popularity of each different video game mode (singleplayer, multiplayer, online co-op) on PC change between the pre-COVID period (2018-2019), the COVID period (2020-2021), and post-COVID period (2022-2023) among the top 250 player-count Steam games from each year from each mode?\n",
    "\n",
    "where we are defining **popularity** by metrics of:\n",
    "- Average concurrent player count over a given period \n",
    "- Peak player count over a given period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe there will be a dramatic rise in the popularity of online co-op and multiplayer games during the COVID era, with some of that increase continuing after COVID. Our thinking is as follows: people were stuck inside and had largely lost the ability to connect with each other in person, so games that allowed online interaction became more appealing. In this study, popularity will be measured using average concurrent player count and peak player count, and we will examine these patterns within the top 250 Steam games across the pre-COVID period (2018-2019), COVID period (2020-2021), and post-COVID period (2022-2023). We also expect that, within the top 250, the number of games tagged as multiplayer or online co-op will increase during COVID compared with pre-COVID, though we recognize that tags are not mutually exclusive and a game may appear in more than one mode. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets.\n",
    "\n",
    "#### Dataset 1: Steam Charts Historical Player Activity\n",
    "\n",
    "#### Dataset name: Steam Charts game-level player activity (scraped/collected)\n",
    "Link to dataset source: https://steamcharts.com/\n",
    "\n",
    "Number of observations: [1463] (roughly: number of games × number of months in 2018–2023, not recounting for repeat games)\n",
    "\n",
    "Number of variables: [144000 total] (year, rank, name, appid, month, avg_players, peak_players, status) per game per month for 6 years\n",
    "\n",
    "This dataset is the core time-series source for our project because it contains the two popularity metrics we can reliably measure across all periods: average concurrent players and peak concurrent players. In practical terms, average concurrent players captures the typical number of people actively playing a game at the same time during a month, while peak concurrent players captures the maximum simultaneous activity reached during that month. Both metrics are counts of players (not percentages), and both are useful: average concurrency reflects sustained engagement, while peak concurrency reflects major surges and maximum demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "# (!) Our data comes from scraping HTML so this section is unneeded.\n",
    "# import get_data # this is where we get the function we need to download data\n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "# datafiles = [\n",
    "#     { 'url': '', 'filename':''},\n",
    "#     { 'url': '', 'filename':''}\n",
    "# ]\n",
    "# get_data.get_raw(datafiles,destination_directory='data/00-raw/')\n",
    "\n",
    "\n",
    "# OUR IMPORTS\n",
    "# Setup imports for Dataset 1 pipeline\n",
    "import csv\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n",
    "\n",
    "### SteamCharts Player Data Collection\n",
    "For this project, the most relevant variables are: game identifier (name and appid), date (month and year), average concurrent players, and peak concurrent players. We may later aggregate monthly values into three study periods: pre-COVID (2018–2019), COVID (2020–2021), and post-COVID (2022–2023). This allows direct period-to-period comparisons for each game and for groups of games by mode tags.\n",
    "\n",
    "A key strength of this dataset is that it provides consistent and public Steam activity data at scale. The main shortcomings are that it is Steam-only (not representative of console ecosystems), may underrepresent edge cases where historical coverage is incomplete for certain titles, and does not directly provide causal explanations for changes in player activity. Also, top-game selection introduces survivorship/popularity bias relative to the full Steam catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Getting Game IDs\n",
    "Our second dataset is Steam250, but due to the modular way our team split up our work, we need to reference the top 250 Steam appids from Steam250 before going to SteamCharts to pull player data. So, we're going to collect those ids into CSVs (where one year = one CSV) to reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build yearly top-250 appid CSVs from Steam250 \n",
    "years = [2018, 2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "for year in years:\n",
    "    url = 'https://steam250.com/reviews/' + str(year)\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    section = soup.select_one(\"section.applist.compact\")\n",
    "    rows = section.find_all(\"div\", id=True)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            rank_div = row.find_all(\"div\", recursive=False)[0]\n",
    "\n",
    "            # Ignore rank change stats e.g. \"+2\" or \"-1\"\n",
    "            texts = [t.strip() for t in rank_div.contents if isinstance(t, str) and t.strip()]\n",
    "            if texts:\n",
    "                rank = int(texts[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            name_tag = row.select_one(\"a[title]\")\n",
    "            name = name_tag.text.strip()\n",
    "\n",
    "            app_link = name_tag[\"href\"]\n",
    "            appid = int(app_link.split(\"/\")[-1])\n",
    "\n",
    "            rating = row.select_one(\"span.rating\").text.strip().replace(\"%\",\"\")\n",
    "            rating = float(rating)\n",
    "\n",
    "            votes_raw = row.select_one(\"span.votes\")[\"title\"]\n",
    "            votes = int(votes_raw.split()[0].replace(\",\", \"\"))\n",
    "\n",
    "            records.append({\n",
    "                \"rank\": rank,\n",
    "                \"name\": name,\n",
    "                \"appid\": appid\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    curr_df = pd.DataFrame(records)\n",
    "    curr_file_name = 'data/02-processed/' + str(year) + '_top250_ids.csv' \n",
    "\n",
    "    curr_df.to_csv(curr_file_name, index=False)\n",
    "\n",
    "# Done message\n",
    "print(\"Top-250 appid files generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the data. First, let's store the results from the previous block into variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018ids = pd.read_csv('data/02-processed/2018_top250_ids.csv')\n",
    "df_2019ids = pd.read_csv('data/02-processed/2019_top250_ids.csv')\n",
    "df_2020ids = pd.read_csv('data/02-processed/2020_top250_ids.csv')\n",
    "df_2021ids = pd.read_csv('data/02-processed/2021_top250_ids.csv')\n",
    "df_2022ids = pd.read_csv('data/02-processed/2022_top250_ids.csv')\n",
    "df_2023ids = pd.read_csv('data/02-processed/2023_top250_ids.csv')\n",
    "\n",
    "yearly_dfs = [df_2018ids, df_2019ids, df_2020ids, df_2021ids, df_2022ids, df_2023ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code ensures a schema of `Steam250IDs(rank, name, appid)` for each CSV. While we're confident in the data's cleanliness and tidiness, we can check and summarize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6): \n",
    "    curr_df = yearly_dfs[i]\n",
    "    print(\n",
    "        'Current year data frame: ',\n",
    "        str(2018 + i),\n",
    "        '\\n===================================================\\n',\n",
    "        '- Shape: ',\n",
    "        curr_df.shape,\n",
    "        '\\n- Do we have 250 games? ',\n",
    "        '(Yes)' if (curr_df['appid'].nunique() == len(curr_df)) else '(No)',\n",
    "        '\\n- How many duplicate games in the current year? (',\n",
    "        curr_df.duplicated().sum(),\n",
    "        ')\\n- Are there any nulls present?\\n',\n",
    "        curr_df.isna().any(),\n",
    "        '\\n\\n',\n",
    "        '- What are the column types?\\n',\n",
    "        curr_df.dtypes,\n",
    "        '\\n\\n',\n",
    "        '- First five rows of the data:\\n',\n",
    "        curr_df.head(),\n",
    "        '\\n\\n',\n",
    "        sep=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks as expected, so we can pull from SteamCharts now that we know what games to look for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SteamCharts collection helpers + pipeline \n",
    "\n",
    "# Base URL pattern for SteamCharts game pages\n",
    "# inject app_id into {app_id}, e.g. app_id=730 -> https://steamcharts.com/app/730\n",
    "BASE_URL = \"https://steamcharts.com/app/{appid}\"\n",
    "\n",
    "# Apprently some sites block requests that do not provide a browser-like user agent.\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; SteamChartsYearScraper/1.0)\"\n",
    "}\n",
    "\n",
    "# Some utlity helpers! ==========================================================================\n",
    "\n",
    "def clean_num(value: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Convert numeric text to float.\n",
    "\n",
    "    Handles:\n",
    "    - commas: \"12,345.6\" -> 12345.6\n",
    "    - blanks/dashes -> None\n",
    "    - invalid values -> None\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "\n",
    "    text = str(value).strip().replace(\",\", \"\")\n",
    "    if text in {\"\", \"-\", \"—\"}:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_game_url(appid: int) -> str:\n",
    "    \"\"\"\n",
    "    Build SteamCharts URL for one appid.\n",
    "    Example: appid=730 -> \"https://steamcharts.com/app/730\"\n",
    "    \"\"\"\n",
    "    return BASE_URL.format(appid=int(appid))\n",
    "\n",
    "\n",
    "\n",
    "def cache_key_for_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Build deterministic cache filename key from URL.\n",
    "    Using md5 keeps filenames short and filesystem-safe.\n",
    "    Why this exists:\n",
    "    - URL text may not be ideal as a filename.\n",
    "    - Hash gives stable and filesystem-safe names.\n",
    "    \"\"\"\n",
    "    return hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "# Input and loading validation ==================================================================\n",
    "\n",
    "\n",
    "def load_input_csv(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and validate one input CSV.\n",
    "    (Probably not neccessary, but I think it's good practice just in case)\n",
    "\n",
    "    Required columns:\n",
    "    - rank\n",
    "    - name\n",
    "    - appid\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned DataFrame with normalized dtypes:\n",
    "      rank:int, name:str, appid:int\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # Read CSV (consider encoding=\"utf-8-sig\")\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Validate required columns\n",
    "    required = {\"rank\", \"name\", \"appid\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"{file_path.name} is missing required columns: {sorted(missing)}. \"\n",
    "            f\"Found columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Keep only needed columns in predictable order \n",
    "    df = df[[\"rank\", \"name\", \"appid\"]].copy()\n",
    "\n",
    "    # Convert numeric fields and fail LOUDLY if invalid\n",
    "    df[\"rank\"] = pd.to_numeric(df[\"rank\"], errors=\"raise\").astype(int)\n",
    "    df[\"appid\"] = pd.to_numeric(df[\"appid\"], errors=\"raise\").astype(int)\n",
    "\n",
    "    # 4) Strip whitespace on name\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.strip()\n",
    "\n",
    "    # Remove rows with empty names \n",
    "    df = df[df[\"name\"] != \"\"].copy()\n",
    "\n",
    "    # drop duplicates on rank+appid\n",
    "    df = df.drop_duplicates(subset=[\"rank\", \"appid\"]).reset_index(drop=True)\n",
    "\n",
    "    # Return cleaned DataFrame\n",
    "    # Sort by rank for deterministic processing\n",
    "    df = df.sort_values(\"rank\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Network + cache =================================================================================\n",
    "\n",
    "\n",
    "def get_game_page_html(\n",
    "    appid: int,\n",
    "    session: requests.Session,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return HTML for one game page, using cache when available.\n",
    "\n",
    "    Flow:\n",
    "    1) Build game URL from appid\n",
    "    2) Compute cache filename from URL hash\n",
    "    3) If cache exists and use_cache=True -> return cached HTML\n",
    "    4) Else fetch from network, save cache, sleep briefly, return HTML\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Ensure cache_dir exists\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 2) Compute URL and cache filename\n",
    "    url = build_game_url(appid)\n",
    "    cache_file = cache_dir / f\"{cache_key_for_url(url)}.html\"\n",
    "\n",
    "    # 3) If cache hit and use_cache: read + return\n",
    "    if cache_file.exists() and use_cache:\n",
    "        return cache_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Live request path\n",
    "    resp = session.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    html = resp.text\n",
    "\n",
    "    # Save to cache for future runs\n",
    "    cache_file.write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "    # Don't attack our lord and savior GabeN with rapid-fire requests\n",
    "    time.sleep(request_delay_sec)\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "# HTML parsing ===================================================================================\n",
    "\n",
    "\n",
    "def parse_year_data_from_html(html: str, target_year: int) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parse SteamCharts monthly table from one app page, filtered to target year.\n",
    "\n",
    "    Input:\n",
    "    - html: raw page HTML\n",
    "    - target_year: year to keep (e.g., 2021)\n",
    "\n",
    "    Output row shape:\n",
    "    {\n",
    "      \"month\": \"YYYY-MM\",\n",
    "      \"avg_players\": float|None,\n",
    "      \"peak_players\": float|None\n",
    "    }\n",
    "\n",
    "    Why this exists:\n",
    "    - Pure parser function (HTML in -> structured rows out).\n",
    "    - Easy to test independently from I/O.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Parse html with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    parsed_rows = []\n",
    "\n",
    "    # 2) Select monthly table rows (table.common-table tbody tr)\n",
    "    rows = soup.select(\"table.common-table tbody tr\")\n",
    "\n",
    "    # 3) For each row, parse month/avg/peak\n",
    "    for tr in rows: \n",
    "        tds = tr.find_all(\"td\")\n",
    "\n",
    "        # Monthly rows should have at least 5 columns\n",
    "        # Month | Avg. Players | Gain | Gain % | Peak Players\n",
    "        if len(tds) < 5:\n",
    "            continue\n",
    "\n",
    "        month_text = tds[0].get_text(\" \", strip=True)\n",
    "\n",
    "        # 4) Skip \"Last 30 Days\"\n",
    "        if month_text.lower() == \"last 30 days\":\n",
    "            continue\n",
    "\n",
    "        # 5) Parse month text with pd.to_datetime(..., format=\"%B %Y\")\n",
    "        month_dt = pd.to_datetime(month_text, format=\"%B %Y\", errors=\"coerce\")\n",
    "        if pd.isna(month_dt):\n",
    "            continue\n",
    "\n",
    "        # 6) Keep rows where parsed year == target_year\n",
    "        if int(month_dt.year) != int(target_year):\n",
    "            continue\n",
    "\n",
    "        # 7) Clean numeric fields with clean_num()\n",
    "        avg_text = tds[1].get_text(\" \", strip=True)\n",
    "        peak_text = tds[4].get_text(\" \", strip=True)\n",
    "\n",
    "        # 8) Return rows sorted by month asc\n",
    "        parsed_rows.append({\n",
    "            \"month\": month_dt.strftime(\"%Y-%m\"),\n",
    "            \"avg_players\": clean_num(avg_text),\n",
    "            \"peak_players\": clean_num(peak_text),\n",
    "        })\n",
    "\n",
    "    # Keep output in chronological order\n",
    "    parsed_rows.sort(key=lambda r: r[\"month\"])\n",
    "    return parsed_rows\n",
    "\n",
    "\n",
    "\n",
    "# year collection ====================================================================================\n",
    "\n",
    "\n",
    "def collect_one_year(\n",
    "    input_csv: Path,\n",
    "    year: int,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect SteamCharts data for one year's input list.\n",
    "\n",
    "    Steps:\n",
    "    - Load CSV (rank, name, appid)\n",
    "    - For each game:\n",
    "      - Fetch or read cached HTML\n",
    "      - Parse target-year monthly rows\n",
    "      - Emit result rows with status labels\n",
    "\n",
    "    Status values:\n",
    "    - \"ok\"               : parsed monthly rows exist\n",
    "    - \"no_data_for_year\" : page loaded, but no rows for that year\n",
    "    - \"request_error\"    : failed HTTP request\n",
    "    - \"parse_error\"      : page fetched but parse failed\n",
    "    \"\"\"\n",
    "\n",
    "    # games_df = load_input_csv(input_csv)\n",
    "    games_df = load_input_csv(input_csv)\n",
    "\n",
    "    # Initialize out_rows = []\n",
    "    out_rows = []\n",
    "    total = len(games_df)\n",
    "\n",
    "    # Create requests.Session()\n",
    "    with requests.Session() as session:\n",
    "        for idx, row in games_df.iterrows():\n",
    "            rank = int(row[\"rank\"])\n",
    "            name = row[\"name\"]\n",
    "            appid = int(row[\"appid\"])\n",
    "\n",
    "            # Fetch HTML (cache first)\n",
    "            try: \n",
    "                html = get_game_page_html(\n",
    "                    appid=appid,\n",
    "                    session=session,\n",
    "                    cache_dir=cache_dir,\n",
    "                    use_cache=use_cache,\n",
    "                    request_delay_sec=request_delay_sec,\n",
    "                )\n",
    "            except Exception:\n",
    "                out_rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"rank\": rank,\n",
    "                    \"name\": name,\n",
    "                    \"appid\": appid,\n",
    "                    \"month\": None,\n",
    "                    \"avg_players\": None,\n",
    "                    \"peak_players\": None,\n",
    "                    \"status\": \"request_error\",\n",
    "                })\n",
    "                print(f\"[{idx+1}/{total}] {name} (appid={appid}): request error\")\n",
    "                continue\n",
    "\n",
    "            # Parse only rows for target year\n",
    "            try:\n",
    "                parsed_rows = parse_year_data_from_html(html, target_year=year)\n",
    "            except Exception:\n",
    "                out_rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"rank\": rank,\n",
    "                    \"name\": name,\n",
    "                    \"appid\": appid,\n",
    "                    \"month\": None,\n",
    "                    \"avg_players\": None,\n",
    "                    \"peak_players\": None,\n",
    "                    \"status\": \"parse_error\",\n",
    "                })\n",
    "                print(f\"[{idx+1}/{total}] {name} (appid={appid}): parse error\")\n",
    "                continue\n",
    "\n",
    "            # no rows found for this yeaer \n",
    "            if not parsed_rows:\n",
    "                out_rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"rank\": rank,\n",
    "                    \"name\": name,\n",
    "                    \"appid\": appid,\n",
    "                    \"month\": None,\n",
    "                    \"avg_players\": None,\n",
    "                    \"peak_players\": None,\n",
    "                    \"status\": \"no_data_for_year\",\n",
    "                })\n",
    "                print(f\"[{idx+1}/{total}] {name} (appid={appid}): no data for year\")\n",
    "                continue\n",
    "\n",
    "            # Found rows: attach metadata \n",
    "            for pr in parsed_rows:\n",
    "                out_rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"rank\": rank,\n",
    "                    \"name\": name,\n",
    "                    \"appid\": appid,\n",
    "                    \"month\": pr[\"month\"],\n",
    "                    \"avg_players\": pr[\"avg_players\"],\n",
    "                    \"peak_players\": pr[\"peak_players\"],\n",
    "                    \"status\": \"ok\",\n",
    "                })\n",
    "\n",
    "            print(f\"[{idx+1}/{total}] {name} ({appid}) -> ok ({len(parsed_rows)} months)\")\n",
    "\n",
    "    result_df = pd.DataFrame(\n",
    "        out_rows,\n",
    "        columns=[\n",
    "            \"year\", \n",
    "            \"rank\", \n",
    "            \"name\", \n",
    "            \"appid\",\n",
    "            \"month\", \n",
    "            \"avg_players\", \n",
    "            \"peak_players\",\n",
    "            \"status\",\n",
    "        ],)\n",
    "\n",
    "    # Sort for readability (rank, then month)\n",
    "    result_df = result_df.sort_values(\n",
    "                    by=[\"rank\", \"month\"], \n",
    "                    na_position=\"last\"\n",
    "                ).reset_index(drop=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def collect_year_range(\n",
    "    start_year: int,\n",
    "    end_year: int,\n",
    "    input_dir: Path,\n",
    "    input_pattern: str,      # e.g. \"{year}_top250_ids.csv\"\n",
    "    output_dir: Path,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    "    write_combined: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run collection across a year range using predictable filenames.\n",
    "\n",
    "    For each year:\n",
    "    - Build input file path from input_pattern\n",
    "    - Skip year if file missing\n",
    "    - Collect year data\n",
    "    - Write per-year CSV\n",
    "\n",
    "    Optionally:\n",
    "    - Combine all years into one DataFrame + CSV\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output_dir exists\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_parts = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "    #   build input_csv path from pattern\n",
    "        input_csv = input_dir / input_pattern.format(year=year)\n",
    "\n",
    "    #   if missing file: print skip and continue\n",
    "        if not input_csv.exists():\n",
    "            print(f\"{year} SKIP - missing input file:{input_csv}\")\n",
    "            continue\n",
    "\n",
    "        year_df = collect_one_year(\n",
    "            input_csv=input_csv,\n",
    "            year=year,\n",
    "            cache_dir=cache_dir,\n",
    "            use_cache=use_cache,\n",
    "            request_delay_sec=request_delay_sec,\n",
    "        )\n",
    "    #   write year_df to output_dir / f\"steamcharts_{year}_top250.csv\"\n",
    "        year_out_path = output_dir / f\"steamcharts_{year}_top250.csv\"\n",
    "        year_df.to_csv(year_out_path, index=False)\n",
    "        print(f\"[{year}] wrote {year_out_path} ({len(year_df)} rows)\")\n",
    "\n",
    "        status_counts = year_df[\"status\"].value_counts(dropna=False)\n",
    "        print(f\"[{year}] status summary:\\n{status_counts.to_string()}\")\n",
    "\n",
    "    #   append year_df to all_parts\n",
    "        all_parts.append(year_df)\n",
    "\n",
    "    # If nothing processed, return empty with expected schema\n",
    "    if not all_parts:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"year\", \"rank\", \"name\", \"appid\", \"month\",\n",
    "            \"avg_players\", \"peak_players\", \"status\"\n",
    "        ])\n",
    "\n",
    "    combined_df = pd.concat(all_parts, ignore_index=True)\n",
    "\n",
    "    if write_combined:\n",
    "        combined_out_path = output_dir / f\"steamcharts_{start_year}_{end_year}_combined.csv\"\n",
    "        combined_df.to_csv(combined_out_path, index=False)\n",
    "        print(f\"\\nWrote combined file: {combined_out_path} ({len(combined_df)} rows)\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# MAIN =======================================================================================================\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Central run configuration.\n",
    "\n",
    "    Keep all user-editable settings here so the rest of the code\n",
    "    stays stable and easy to reason about.\n",
    "    \"\"\"\n",
    "\n",
    "    start_year = 2018\n",
    "    end_year = 2023\n",
    "\n",
    "    # IMPORTANT!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # PAY ATTENTION TO ME!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # CHANGE THESE PATHS AS NEEDED TO POINT TO CORRECT FOLDERS AND FILES\n",
    "    # I HAVE PUT EXAMPLE PATHS AS COMMENTS BUT I DONT KNOW WHERE WE ACTUALLY WANT EVERYTHING TO GO\n",
    "    input_dir = Path(\".\") # e.g. Path(\"data/00-raw\")\n",
    "\n",
    "    # File name pattern for input CSVs, with {year} placeholder, e.g. \"2018_top250_ids.csv\"\n",
    "    input_pattern = \"{year}_top250_ids.csv\"\n",
    "\n",
    "    # Output and cache directory for per-year and combined CSVs\n",
    "    output_dir = Path(\"outputs\")  # e.g. Path(\"data/01-interim\")\n",
    "    cache_dir = Path(\"steamcharts_cache\") # e.g. Path(\"data/01-interim/steamcharts_cache\")\n",
    "\n",
    "    # Cache behavior: True = reuse cached pages if present\n",
    "    use_cache = True\n",
    "\n",
    "    # Delay for live requests (in seconds) to avoid hammering the server\n",
    "    request_delay_sec = 0.6\n",
    "\n",
    "    combined_df = collect_year_range(\n",
    "        start_year=start_year,\n",
    "        end_year=end_year,\n",
    "        input_dir=input_dir,\n",
    "        input_pattern=input_pattern,\n",
    "        output_dir=output_dir,\n",
    "        cache_dir=cache_dir,\n",
    "        use_cache=use_cache,\n",
    "        request_delay_sec=request_delay_sec,\n",
    "        write_combined=True,\n",
    "    )\n",
    "\n",
    "    print(\"\\nCombined preview:\")\n",
    "    print(combined_df.head(20).to_string(index=False))\n",
    "\n",
    "    print(\"\\nCombined status summary:\")\n",
    "    if not combined_df.empty:\n",
    "        print(combined_df[\"status\"].value_counts(dropna=False).to_string())\n",
    "    else:\n",
    "        print(\"No data collected. Check input file paths/pattern.\")\n",
    "\n",
    "\n",
    "# Run collection\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some simple stats looking at the combined data from steamcharts_2018_2023_combined.csv\n",
    "\n",
    "# CHANGE THIS PATH if needed\n",
    "combined_path = Path(\"00-raw/outputs/steamcharts_2018_2023_combined.csv\")\n",
    "\n",
    "df = pd.read_csv(combined_path)\n",
    "\n",
    "# Columns\n",
    "print(\"=== Columns ===\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Overall size\n",
    "print(\"\\n=== Overall Size ===\")\n",
    "print(f\"Rows: {df.shape[0]}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "\n",
    "# Status counts\n",
    "print(\"\\n=== Status Counts ===\")\n",
    "if \"status\" in df.columns:\n",
    "    print(df[\"status\"].value_counts(dropna=False).to_string())\n",
    "else:\n",
    "    print(\"No 'status' column found.\")\n",
    "\n",
    "# Period-level summary (descriptive only)\n",
    "required = {\"year\", \"avg_players\", \"peak_players\"}\n",
    "if required.issubset(df.columns):\n",
    "    tmp = df.copy()\n",
    "\n",
    "    # light coercion for summary calculations only\n",
    "    tmp[\"year\"] = pd.to_numeric(tmp[\"year\"], errors=\"coerce\")\n",
    "    tmp[\"avg_players\"] = pd.to_numeric(tmp[\"avg_players\"], errors=\"coerce\")\n",
    "    tmp[\"peak_players\"] = pd.to_numeric(tmp[\"peak_players\"], errors=\"coerce\")\n",
    "\n",
    "    # use only successful scrape rows if status exists\n",
    "    if \"status\" in tmp.columns:\n",
    "        tmp = tmp[tmp[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "    def period_label(y):\n",
    "        if pd.isna(y):\n",
    "            return \"unknown\"\n",
    "        y = int(y)\n",
    "        if 2018 <= y <= 2019:\n",
    "            return \"pre_covid\"\n",
    "        elif 2020 <= y <= 2021:\n",
    "            return \"covid\"\n",
    "        elif 2022 <= y <= 2023:\n",
    "            return \"post_covid\"\n",
    "        return \"other\"\n",
    "\n",
    "    tmp[\"period\"] = tmp[\"year\"].apply(period_label)\n",
    "\n",
    "    period_summary = (\n",
    "        tmp.groupby(\"period\", as_index=False)\n",
    "           .agg(\n",
    "               n_rows=(\"period\", \"size\"),\n",
    "               avg_players_mean=(\"avg_players\", \"mean\"),\n",
    "               avg_players_median=(\"avg_players\", \"median\"),\n",
    "               peak_players_mean=(\"peak_players\", \"mean\"),\n",
    "               peak_players_median=(\"peak_players\", \"median\"),\n",
    "           )\n",
    "           .sort_values(\"period\")\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Period-Level Summary (status='ok') ===\")\n",
    "    print(period_summary.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nMissing one or more required columns for period summary:\", required)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team expectations are also [found separately here](./admin/rules.md).\n",
    "\n",
    "### Communication\n",
    "**Discord** is our main form of communication. We have a group chat.\n",
    "- **Responding** If there is a message that needs responding to/acknowledgement, respond within about 1 day/24 hours (48 hours can be acceptable if there's an emergency). The exception to this if we have a planned meeting coming up and we ask for a ready check, then it's expected that there's an almost immediate response.\n",
    "- **Respect** Stay reasonably respectful to one another. It's okay to disagree, but do talk about the issue together or bring in another person (or the entire group) to discuss the matter if needed to mediate. If you don't talk about something, there's no way we'd know what's wrong.\n",
    "  \n",
    "### Missing Tasks/Meetings\n",
    "- **Tasks** If you can't complete a task, let us know as soon as possible (i.e. as soon as you find out) so we can reorganize task assignment or move our schedule around.\n",
    "- **Meetings** If you can't make a meeting, that's okay, and it's not detrimental. However, that would mean you can't provide your input on something live. You can share your thoughts and ideas in our group chat in this event so we can discuss your ideas. We do take meeting notes, so please read them to stay up to date with the team.\n",
    "\n",
    "### Team Structure and Decision Making\n",
    "- **Team Roles** We don't plan on having established team roles, but we'll try to have everyone do a bit of everything (to the best of our ability). The only real \"role\" we'll have is one note taker per meeting.\n",
    "- **Task Tracking** We'll use the GitHub Projects tab/Kanban on the team repository. \n",
    "- **Decision Making** If it comes to a decision, we'll have a vote to decide (more votes = win).\n",
    "\n",
    "### Addressing Problem Members\n",
    "This is our protocol on addressing non-responsive teammates/those refusing to do work:\n",
    "1. First offense: check-in and see if everything is okay.\n",
    "2. Second offense: what we do depends, but we'll talk with you again.\n",
    "3. Clearly becoming a pattern: talk to a TA and/or the professor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Type | Date | Meeting/Due Time | To Complete Before Meeting | Discuss at Meeting |\n",
    "| ---- | ---- | ---- | ---- |  ---- |\n",
    "| Meeting | 2/22 | 2pm  | Read up on EDA checkpoint requirements; come into meeting with ideas on how to approach things | Discuss EDA and split up tasks for EDA checkpoint. |\n",
    "| Meeting | 3/1  | 2pm  | Make 70-80% progress on EDA tasks | Check in on EDA progress and see what needs to be done. |\n",
    "| **DUE** EDA Checkpoint | 3/4 | 11:59pm | - | - |\n",
    "| Meeting | 3/8  | 2pm  | Wrap up any loose ends we didn't finish (if applicable). Read up on final project expectations. | Discuss final project tasks and split up tasks. |\n",
    "| Meeting | 3/15 | 2pm  | Make about 80% progress on final project tasks. | Discuss the video work. |\n",
    "| **DUE** Final Project + Video | 3/18 | 11:59pm | - | - |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COGS108_FA25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
