{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "Team:\n",
    "- Ashley Vo: Conceptualization, Writing – review & editing, Project administration, Data curation\n",
    "- Dorje Pradhan: Conceptualization, Writing – original draft, Writing – review & editing, Data curation\n",
    "- Kilhoon (Andy) Kim: Writing – original draft, Writing – review & editing, Data curation\n",
    "- Kobe Wood: Data curation, Writing – original draft, Writing – review & editing\n",
    "- Vy (Kiet) Dang: Background research, Writing – original draft, Data curation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the popularity of each different video game mode (singleplayer, multiplayer, online co-op) on PC change between the pre-COVID period (2018-2019), the COVID period (2020-2021), and post-COVID period (2022-2023) among the top 250 player-count Steam games from each year from each mode?\n",
    "\n",
    "where we are defining **popularity** by metrics of:\n",
    "- Average concurrent player count over a given period \n",
    "- Peak player count over a given period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global coronavirus outbreak in 2020, called COVID-19, has caused a global pandemic, forcing people to isolate and quarantine from each other. During the lockdown period, people spent most of their time at home and turned to digital entertainment and video games as a way to socialize and de-stress. Therefore, the gaming industry in this period witnessed a peak in gamer activity, play time, sales, and stock values.\n",
    "\n",
    "In this study, we are interested in finding which games were the most popular from each game mode (single player, multiplayer, online co-op). Seeing what game mode people are most interested in, could give the gaming industry a better understanding of their consumer's desires. At first, the group was interested in researching the top 50 games from each game mode, between the pre-COVID period (2018-2019), the COVID era (2020-2021), and the post-COVID era (2021-2023), across multiple popular platforms such as Steam, Epic Games, Xbox, PlayStation, Nintendo, etc. **However**, not all platforms share their players' statistics to the public. So, we narrowed down our platform domain to only include Steam because it has a public database which is called \"Steam Charts\" and \"SteamDB\".\n",
    "\n",
    "There are multiple research papers conducted about video games' activity and price range analysis over the course of time, such as:\n",
    "- Aliev et al. (2025): These researchers investigated how the pandemic affected the prices and player reviews of mostly Indie games on Steam. By analyzing SteamDB data, they found that player reviews and activity levels are highly correlated. This study confirmed that SteamDB is a reliable tool for our project, but their work mostly focused on Indie games category while treating AAA - a pricing category - as its own game category, but also, we want to look at the top 50 games overall from each game mode.\n",
    "\n",
    "- Şener et al. (2021): This paper successfully investigated the broader economic impact of COVID-19 on the gaming industry and showed a significant rise in player activity on Steam during 2020. Their findings gave us a \"baseline\" of when player activity started to increase, but it did not include other factors like playtime, pricing dynamics, game modes, etc. \n",
    "\n",
    "- Toledo (2021): Toledo used consumer **surveys** to study how gaming habits changed during lockdown. The survey showed that games became a bridge to \"online social life\" during quarantine period. This findings is why we emphasize on comparing Online Co-op and Multiplayer modes against Single Player games, as we wanted to see if that online social trend lasted after the pandemic ends. This is the harder data to categorize due to Steam tags are very misleading by mixing Singleplayers and Multiplayers altogether.\n",
    "\n",
    "Therefore, as curious gamers and data analysts, we decided to conduct extensive research on what are the top 50 popular games from each game mode on Steam pre, during, and post the COVID-19 era.\n",
    "\n",
    "References (include links):\n",
    "1. Aliev, A. R., Eyniyev, R., & Aliyev, T. A. (n.d.). Analyzing Price Dynamics, Activity of Players and Reviews of Popular Indie Games on Steam Post-COVID-19 Pandemic using SteamDB. https://www.mecs-press.org/ijitcs/ijitcs-v17-n3/IJITCS-V17-N3-3.pdf\n",
    "2. Şener, Mehmet & Yalcin, Turkan & Gulseven, Osman. (2021). The Impact of COVID-19 on the Video Game Industry. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3766147\n",
    "3. Toledo, M. (2021). Video Game Habits COVID-19. Journal of Marketing Management and Consumer Behavior, 3(4), 66–89. https://doi.org/10.2139/ssrn.3676004"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe there will be a dramatic rise in the popularity of online co-op and multiplayer games during the COVID era, with some of that increase continuing after COVID. Our thinking is as follows: people were stuck inside and had largely lost the ability to connect with each other in person, so games that allowed online interaction became more appealing. In this study, popularity will be measured using average concurrent player count and peak player count, and we will examine these patterns within the top 250 Steam games across the pre-COVID period (2018-2019), COVID period (2020-2021), and post-COVID period (2022-2023). We also expect that, within the top 250, the number of games tagged as multiplayer or online co-op will increase during COVID compared with pre-COVID, though we recognize that tags are not mutually exclusive and a game may appear in more than one mode. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "#### Dataset 1: Steam250 - Top 250 Games Of Each Year\n",
    "- Dataset name: Steam250 - Top 250 Games Of Each Year\n",
    "- Link to the dataset:\n",
    "  - 2018: https://steam250.com/2018 \n",
    "  - 2019: https://steam250.com/2019\n",
    "  - 2020: https://steam250.com/2020\n",
    "  - 2021: https://steam250.com/2021\n",
    "  - 2022: https://steam250.com/2022\n",
    "  - 2023: https://steam250.com/2023\n",
    "- Number of observations (per year): 250 \n",
    "- Number of variables (per year): 5\n",
    "- Important notes: Steam250 does not provide play mode data, so we will need to obtain and join this with another dataset, in our case, Dataset 2.\n",
    "\n",
    "#### Dataset 2: Steam Games Dataset (Kaggle)\n",
    "- Dataset name: Steam Games Dataset (Kaggle)\n",
    "- Link to the dataset: https://www.kaggle.com/datasets/fronkongames/steam-games-dataset\n",
    "- Number of observations: 122611\n",
    "- Number of variables: 39\n",
    "- Important notes: This dataset serves as a supplementary dataset to provide playmode data for Dataset 1.\n",
    "\n",
    "#### Dataset 3: Steam Charts Historical Player Activity\n",
    "- Dataset name: Steam Charts game-level player activity (scraped/collected)\n",
    "- Link to the dataset: https://steamcharts.com/\n",
    "- Number of observations: [1463] (roughly: number of games × number of months in 2018–2023, not recounting for repeat games)\n",
    "- Number of variables: [8] (year, rank, name, appid, month, avg_players, peak_players, status)\n",
    "- This dataset is the core time-series source for our project because it contains the two popularity metrics we can reliably measure across all periods: average concurrent players and peak concurrent players. In practical terms, average concurrent players captures the typical number of people actively playing a game at the same time during a month, while peak concurrent players captures the maximum simultaneous activity reached during that month. Both metrics are counts of players (not percentages), and both are useful: average concurrency reflects sustained engagement, while peak concurrency reflects major surges and maximum demand.\n",
    "- Important notes: We will first gather a list of the top 250 games from each year before pulling from SteamCharts to avoid needlessly gathering data we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "# (!) Our data comes from scraping HTML so this section is unneeded.\n",
    "import get_data # this is where we get the function we need to download data\n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "# datafiles = [\n",
    "#     { 'url': '', 'filename':''},\n",
    "#     { 'url': '', 'filename':''}\n",
    "# ]\n",
    "# get_data.get_raw(datafiles,destination_directory='data/00-raw/')\n",
    "\n",
    "\n",
    "# OUR IMPORTS\n",
    "import csv\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Note: this will need to be run to get games.csv (Dataset 2) as the CSV is too large for GitHub.\n",
    "get_data.main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steam250 Data Collection\n",
    "Steam250 is a site that displays a ranking of the top 250 of each year. Although there is no CSV provided, we can scrape the website to gather the data we need. They provide 5 variables of interest to us: \n",
    "1. Rank is an integer that denotes the ranking of each game with 1 being the best of that year and 250 being the 250th best of the year.\n",
    "2. AppID is an integer that uniquely identifies each and every game on Steam.\n",
    "3. Name is a string that belongs to the name of the game.\n",
    "4. Rating is a player voted score. This is an integer but realistically acts as a percentage (e.g. 94 corresponds to 94%).\n",
    "5. Number of votes is an integer that denotes the number of players that have voted for that game.\n",
    "\n",
    "The biggest acknowledgement we have to make is that our analysis is going to be based off of Steam250's definition of top games. However, upon [reading their process](https://steam250.com/about), we concluded that it was an agreeable and good enough approach as finding the \"top games\" isn't readily or easily available elsewhere.\n",
    "\n",
    "Let's load all of the CSVs so we can observe their structure en masse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam250_2018_df = pd.read_csv('data/00-raw/2018_top250.csv')\n",
    "steam250_2019_df = pd.read_csv('data/00-raw/2019_top250.csv')\n",
    "steam250_2020_df = pd.read_csv('data/00-raw/2020_top250.csv')\n",
    "steam250_2021_df = pd.read_csv('data/00-raw/2021_top250.csv')\n",
    "steam250_2022_df = pd.read_csv('data/00-raw/2022_top250.csv')\n",
    "steam250_2023_df = pd.read_csv('data/00-raw/2023_top250.csv')\n",
    "\n",
    "yearly_dfs = [steam250_2018_df, steam250_2019_df, steam250_2020_df, steam250_2021_df, steam250_2022_df, steam250_2023_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure there aren't any missing values and our data types line up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    curr_df = yearly_dfs[i]\n",
    "\n",
    "    print(\n",
    "        '=' * 64, '\\n',\n",
    "        f'Current year data frame: {2018 + i}\\n',\n",
    "        '=' * 64, '\\n',\n",
    "        'Shape: ',\n",
    "        curr_df.shape, '\\n\\n',\n",
    "        'Any nulls?\\n',\n",
    "        curr_df.isna().any(), '\\n\\n',\n",
    "        'Column types:\\n',\n",
    "        curr_df.dtypes, '\\n\\n',\n",
    "        'First five rows of the data:\\n',\n",
    "        curr_df.head(5), '\\n\\n',\n",
    "        sep=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks as we expect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steam Games Dataset\n",
    "This dataset was provided by Martin Bustos on Kaggle. Bustos created the dataset using Steam's API and Steam Spy. It provides meta information of over 122,000 games on Steam. It has 39 variables. However, we are only interested in three key variables while being able to ignore the rest as they are not important to our research question:\n",
    "1. AppID is a integer value that uniquely identifies each and every game on Steam.\n",
    "2. Name is a string that belongs to the name of the game.\n",
    "3. Categories should be an array of strings. Each string corresponds to a characterization of the game such as \"singleplayer\" or \"PvP.\"\n",
    "\n",
    "As we'll discover, reading the CSV proved to be a little troublesome with column mismatch. Since we're only concerned with the Categories of each game this dataset provides, we can safely disregard the other columns. There were other datasets we experimented with that didn't fully account for all of the games we've gathered.\n",
    "\n",
    "First, let's read in the CSV downloaded from Kaggle and get an idea of its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "steam_games_df = pd.read_csv('./data/00-raw/games.csv', index_col=False)\n",
    "\n",
    "steam_games_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a peak at what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is not clean. The columns don't line up with the values in the rows. For example, we see what looks like descriptions of games under \"Supported Languages\" rather than \"About the game.\" In printing out the column types, we see some discrepencies like \"Metacritic url\" being an `int64` rather than an object. Luckily, we only care about a few variables in the dataset: AppID, Name, and Categories. We can ignore the rest. Note that Categories is currently under the \"Genres\" column. We can fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games_subset_df = steam_games_df[['AppID', 'Name', 'Genres']]\n",
    "steam_games_subset_df.columns = ['appid', 'name', 'genres']\n",
    "\n",
    "steam_games_subset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games_subset_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be one entry without a name. Let's find out what game that is and whether it should be cause for concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games_subset_df[steam_games_subset_df['name'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, after [looking up the game on Steam](https://store.steampowered.com/app/396420/_/), it literally has no name, so this isn't something to worry about. Now, we can move on to merging datasets 1 and 2 together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging: Steam250 and Steam Games (Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to merge datasets 1 and 2. There are a few things we must fix like adjusting the categories to what we need. So, we will find out what needs to be replaced. We'll use 2018 as an example to find out what categories we need to map as well as find out if there are any python typing quirks with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam250_2018_df_merged = steam250_2018_df.merge(\n",
    "    steam_games_subset_df,\n",
    "    on='appid',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print('What is the type of the tags column?\\n', type(steam250_2018_df_merged['genres'].iloc[0]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's a string, we should convert it to a list to easily parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam250_2018_df_merged['genres'] = steam250_2018_df_merged['genres'].fillna('').str.split(',')\n",
    "steam250_2018_df_merged\n",
    "# print('What is the type of the tags column now?\\n', type(steam250_2018_df_merged['genres'].iloc[0]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See unique tags\n",
    "unique_tags = (\n",
    "    steam250_2018_df_merged['genres']\n",
    "    .explode()\n",
    "    .str\n",
    "    .strip()\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "print('What are the unique values that can appear under categories?\\n', unique_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two things:\n",
    "- A mapping to group and normalize similar play modes together to the ones that are of our interest to our research question: singleplayer, multiplayer, and co-op\n",
    "- A helper function to clean up the columns in the rows for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = {\n",
    "    'Single-player': 'singleplayer',\n",
    "\n",
    "    'Multi-player': 'multiplayer',\n",
    "    'MMO': 'multiplayer',\n",
    "    'PvP': 'multiplayer',\n",
    "    'Online PvP': 'multiplayer',\n",
    "    'LAN PvP': 'multiplayer',\n",
    "    'Shared/Split Screen PvP': 'multiplayer',\n",
    "    'Cross-Platform Multiplayer': 'multiplayer',\n",
    "\n",
    "    'Co-op': 'co-op',\n",
    "    'Online Co-op': 'co-op',\n",
    "    'LAN Co-op': 'co-op',\n",
    "    'Shared/Split Screen Co-op': 'co-op',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(tag_list):\n",
    "    if not isinstance(tag_list, list):\n",
    "        return []\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    for tag in tag_list:\n",
    "        tag = tag.strip()\n",
    "\n",
    "        if tag in tag_map:\n",
    "            cleaned.append(tag_map[tag])\n",
    "\n",
    "    return list(set(cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can adjust the tags for each top 250 per year in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    curr_year_df = pd.read_csv(f'./data/00-raw/{2018 + i}_top250.csv')\n",
    "\n",
    "    curr_merged_df = curr_year_df.merge(\n",
    "        steam_games_subset_df,\n",
    "        on='appid',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    curr_merged_df = curr_merged_df[['rank', 'appid', 'name_x', 'num_votes', 'rating', 'genres']]\n",
    "    curr_merged_df.columns = ['rank', 'appid', 'name', 'num_votes', 'rating', 'tags']\n",
    "\n",
    "    # Convert the tags column to a list\n",
    "    curr_merged_df['tags'] = (\n",
    "        curr_merged_df['tags']\n",
    "        .fillna('')\n",
    "        .str.split(',')\n",
    "    )\n",
    "\n",
    "    curr_merged_df['tags'] = curr_merged_df['tags'].apply(clean_tags)\n",
    "\n",
    "    # Save to the data/02-processed directory\n",
    "    curr_merged_df.to_csv(f'./data/02-processed/{2018 + i}_top250_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure all of our data is in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6): \n",
    "    curr_df = pd.read_csv(f'./data/02-processed/{2018 + i}_top250_final.csv')\n",
    "    print(\n",
    "        '=' * 64, '\\n',\n",
    "        f'Current year data frame: {2018 + i}\\n',\n",
    "        '=' * 64, '\\n',\n",
    "        'Shape: ',\n",
    "        curr_df.shape, '\\n\\n',\n",
    "        'Any nulls?\\n',\n",
    "        curr_df.isna().any(), '\\n\\n',\n",
    "        'Column types:\\n',\n",
    "        curr_df.dtypes, '\\n\\n',\n",
    "        'First five rows of the data:\\n',\n",
    "        curr_df.head(5), '\\n\\n',\n",
    "        sep=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, everything looks as expected, so we can now look to SteamCharts for more granular data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SteamCharts Player Data Collection\n",
    "For this project, the most relevant variables are: game identifier (name and appid), date (month and year), average concurrent players, and peak concurrent players. \n",
    "- A game name (a string) may not be unique against the entire Steam catalog, so there exists a Steam appid that uniquely identifies each game. This is a number, and it is expected every game has one. \n",
    "- Dates are going to have some format similar to `YYYY-MM` such that we can parse it to aggregate the monthly player data by the defined study periods.\n",
    "- Average and peak concurrent players are numeric values that we expect to be greater than or equal to zero. Since we are looking at the top 250 Steam games, it's expected this value is certainly greater than zero.\n",
    "\n",
    "We may later aggregate monthly values into three study periods: pre-COVID (2018–2019), COVID (2020–2021), and post-COVID (2022–2023). This allows direct period-to-period comparisons for each game and for groups of games by mode tags.\n",
    "\n",
    "A key strength of this dataset is that it provides consistent and public Steam activity data at scale. SteamCharts obtains data directly from Steam's Web API. The main shortcomings are that it is Steam-only therefore not representative of console ecosystems like Nintendo, Sony, and Xbox's. This may lead to underrepresention and edge cases where historical coverage is incomplete for certain titles and does not directly provide causal explanations for changes in player activity. It is also important to note Steam does not record data for players who decide to play offline i.e. disconnected from the internet therefore disconnected from Steam's servers. As such, the data does not account for these cases even if players are playing through Steam. Lastly, the top-game selection introduces survivorship/popularity bias relative to the full Steam catalog, leaving out games that may see interesting growth or variability in their player populations despite not being in the top 250."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions Overview\n",
    "The helper functions below standardize small repeated tasks so the collection pipeline is easier to read and debug.  \n",
    "- `clean_num` converts numeric text scraped from HTML (including commas and dash placeholders) into floats.  \n",
    "- `build_game_url` creates a SteamCharts URL from a game `appid`.  \n",
    "- `cache_key_for_url` creates a deterministic filename-safe cache key from a URL so cached HTML pages can be reused across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SteamCharts collection helpers + pipeline \n",
    "\n",
    "# Base URL pattern for SteamCharts game pages\n",
    "# inject app_id into {app_id}, e.g. app_id=730 -> https://steamcharts.com/app/730\n",
    "BASE_URL = \"https://steamcharts.com/app/{appid}\"\n",
    "\n",
    "# Apprently some sites block requests that do not provide a browser-like user agent.\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; SteamChartsYearScraper/1.0)\"\n",
    "}\n",
    "\n",
    "# Some utlity helpers! ==========================================================================\n",
    "\n",
    "def clean_num(value: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Convert numeric text to float.\n",
    "\n",
    "    Handles:\n",
    "    - commas: \"12,345.6\" -> 12345.6\n",
    "    - blanks/dashes -> None\n",
    "    - invalid values -> None\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "\n",
    "    text = str(value).strip().replace(\",\", \"\")\n",
    "    if text in {\"\", \"-\", \"—\"}:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_game_url(appid: int) -> str:\n",
    "    \"\"\"\n",
    "    Build SteamCharts URL for one appid.\n",
    "    Example: appid=730 -> \"https://steamcharts.com/app/730\"\n",
    "    \"\"\"\n",
    "    return BASE_URL.format(appid=int(appid))\n",
    "\n",
    "\n",
    "\n",
    "def cache_key_for_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Build deterministic cache filename key from URL.\n",
    "    Using md5 keeps filenames short and filesystem-safe.\n",
    "    Why this exists:\n",
    "    - URL text may not be ideal as a filename.\n",
    "    - Hash gives stable and filesystem-safe names.\n",
    "    \"\"\"\n",
    "    return hashlib.md5(url.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Loading and Validation\n",
    "Before scraping SteamCharts, we validate each yearly input CSV to ensure it has the expected schema: `rank`, `name`, and `appid`.  \n",
    "This step enforces consistent data types, removes empty names, drops duplicates, and sorts by rank so processing is deterministic.  \n",
    "Failing early on malformed input helps avoid harder-to-diagnose errors later in the scraping pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and loading validation ==================================================================\n",
    "def load_input_csv(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and validate one input CSV.\n",
    "    (Probably not neccessary, but I think it's good practice just in case)\n",
    "\n",
    "    Required columns:\n",
    "    - rank\n",
    "    - name\n",
    "    - appid\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned DataFrame with normalized dtypes:\n",
    "      rank:int, name:str, appid:int\n",
    "    \"\"\"\n",
    "\n",
    "    # Read CSV (consider encoding=\"utf-8-sig\")\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Validate required columns\n",
    "    required = {\"rank\", \"name\", \"appid\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"{file_path.name} is missing required columns: {sorted(missing)}. \"\n",
    "            f\"Found columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Keep only needed columns in predictable order \n",
    "    df = df[[\"rank\", \"name\", \"appid\"]].copy()\n",
    "\n",
    "    # Convert numeric fields and fail LOUDLY if invalid\n",
    "    df[\"rank\"] = pd.to_numeric(df[\"rank\"], errors=\"raise\").astype(int)\n",
    "    df[\"appid\"] = pd.to_numeric(df[\"appid\"], errors=\"raise\").astype(int)\n",
    "\n",
    "    # 4) Strip whitespace on name\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.strip()\n",
    "\n",
    "    # Remove rows with empty names \n",
    "    df = df[df[\"name\"] != \"\"].copy()\n",
    "\n",
    "    # drop duplicates on rank+appid\n",
    "    df = df.drop_duplicates(subset=[\"rank\", \"appid\"]).reset_index(drop=True)\n",
    "\n",
    "    # Return cleaned DataFrame\n",
    "    # Sort by rank for deterministic processing\n",
    "    df = df.sort_values(\"rank\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Requests and HTML Caching\n",
    "The scraper first checks whether a game's HTML page is already cached locally. If so, it reads from cache; otherwise, it fetches the page from SteamCharts and stores a local copy.  \n",
    "This reduces repeated web requests, improves reproducibility across reruns, and speeds up development/testing.  \n",
    "A short delay is added between live requests to avoid overloading the source website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network + cache =================================================================================\n",
    "def get_game_page_html(\n",
    "    appid: int,\n",
    "    session: requests.Session,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return HTML for one game page, using cache when available.\n",
    "\n",
    "    Flow:\n",
    "    1) Build game URL from appid\n",
    "    2) Compute cache filename from URL hash\n",
    "    3) If cache exists and use_cache=True -> return cached HTML\n",
    "    4) Else fetch from network, save cache, sleep briefly, return HTML\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure cache_dir exists\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Compute URL and cache filename\n",
    "    url = build_game_url(appid)\n",
    "    cache_file = cache_dir / f\"{cache_key_for_url(url)}.html\"\n",
    "\n",
    "    # If cache hit and use_cache: read + return\n",
    "    if cache_file.exists() and use_cache:\n",
    "        return cache_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Live request path\n",
    "    resp = session.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    html = resp.text\n",
    "\n",
    "    # Save to cache for future runs\n",
    "    cache_file.write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "    # Don't attack our lord and savior GabeN with rapid-fire requests\n",
    "    time.sleep(request_delay_sec)\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "# HTML parsing ===================================================================================\n",
    "\n",
    "\n",
    "def parse_year_data_from_html(html: str, target_year: int) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parse SteamCharts monthly table from one app page, filtered to target year.\n",
    "\n",
    "    Input:\n",
    "    - html: raw page HTML\n",
    "    - target_year: year to keep (e.g., 2021)\n",
    "\n",
    "    Output row shape:\n",
    "    {\n",
    "      \"month\": \"YYYY-MM\",\n",
    "      \"avg_players\": float|None,\n",
    "      \"peak_players\": float|None\n",
    "    }\n",
    "\n",
    "    Why this exists:\n",
    "    - Pure parser function (HTML in -> structured rows out).\n",
    "    - Easy to test independently from I/O.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse html with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    parsed_rows = []\n",
    "\n",
    "    # Select monthly table rows (table.common-table tbody tr)\n",
    "    rows = soup.select(\"table.common-table tbody tr\")\n",
    "\n",
    "    # For each row, parse month/avg/peak\n",
    "    for tr in rows: \n",
    "        tds = tr.find_all(\"td\")\n",
    "\n",
    "        # Monthly rows should have at least 5 columns\n",
    "        # Month | Avg. Players | Gain | Gain % | Peak Players\n",
    "        if len(tds) < 5:\n",
    "            continue\n",
    "\n",
    "        month_text = tds[0].get_text(\" \", strip=True)\n",
    "\n",
    "        # Skip \"Last 30 Days\"\n",
    "        if month_text.lower() == \"last 30 days\":\n",
    "            continue\n",
    "\n",
    "        # Parse month text with pd.to_datetime(..., format=\"%B %Y\")\n",
    "        month_dt = pd.to_datetime(month_text, format=\"%B %Y\", errors=\"coerce\")\n",
    "        if pd.isna(month_dt):\n",
    "            continue\n",
    "\n",
    "        # Keep rows where parsed year == target_year\n",
    "        if int(month_dt.year) != int(target_year):\n",
    "            continue\n",
    "\n",
    "        # Clean numeric fields with clean_num()\n",
    "        avg_text = tds[1].get_text(\" \", strip=True)\n",
    "        peak_text = tds[4].get_text(\" \", strip=True)\n",
    "\n",
    "        # Return rows sorted by month asc\n",
    "        parsed_rows.append({\n",
    "            \"month\": month_dt.strftime(\"%Y-%m\"),\n",
    "            \"avg_players\": clean_num(avg_text),\n",
    "            \"peak_players\": clean_num(peak_text),\n",
    "        })\n",
    "\n",
    "    # Keep output in chronological order\n",
    "    parsed_rows.sort(key=lambda r: r[\"month\"])\n",
    "    return parsed_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Collection Logic\n",
    "For each year, we read the corresponding top-250 appid file, fetch each game's SteamCharts page, and parse only rows that match the target year.  \n",
    "Each output row is assigned a status:\n",
    "- `ok` if monthly rows were parsed successfully,\n",
    "- `no_data_for_year` if the page loaded but no rows matched the year,\n",
    "- `request_error` for HTTP/network failures,\n",
    "- `parse_error` for HTML parsing issues.\n",
    "This status field helps us audit data quality before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year collection ====================================================================================\n",
    "def collect_one_year(\n",
    "    input_csv: Path,\n",
    "    year: int,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect SteamCharts data for one year's input list.\n",
    "\n",
    "    Steps:\n",
    "    - Load CSV (rank, name, appid)\n",
    "    - For each game:\n",
    "      - Fetch or read cached HTML\n",
    "      - Parse target-year monthly rows\n",
    "      - Emit result rows with status labels\n",
    "\n",
    "    Status values:\n",
    "    - \"ok\"               : parsed monthly rows exist\n",
    "    - \"no_data_for_year\" : page loaded, but no rows for that year\n",
    "    - \"request_error\"    : failed HTTP request\n",
    "    - \"parse_error\"      : page fetched but parse failed\n",
    "    \"\"\"\n",
    "\n",
    "    # games_df = load_input_csv(input_csv)\n",
    "    games_df = load_input_csv(input_csv)\n",
    "\n",
    "    # Initialize out_rows = []\n",
    "    out_rows = []\n",
    "    total = len(games_df)\n",
    "\n",
    "    # Create requests.Session()\n",
    "    with requests.Session() as session:\n",
    "        for idx, row in games_df.iterrows():\n",
    "            rank = int(row[\"rank\"])\n",
    "            name = row[\"name\"]\n",
    "            appid = int(row[\"appid\"])\n",
    "\n",
    "            # Fetch HTML (cache first)\n",
    "            try: \n",
    "                html = get_game_page_html(\n",
    "                    appid=appid,\n",
    "                    session=session,\n",
    "                    cache_dir=cache_dir,\n",
    "                    use_cache=use_cache,\n",
    "                    request_delay_sec=request_delay_sec,\n",
    "                )\n",
    "            except Exception:\n",
    "                out_rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"rank\": rank,\n",
    "                    \"name\": name,\n",
    "                    \"appid\": appid,\n",
    "                    \"month\": None,\n",
    "                    \"avg_players\": None,\n",
    "                    \"peak_players\": None,\n",
    "                    \"status\": \"request_error\",\n",
    "                })\n",
    "                print(f\"[{idx+1}/{total}] {name} (appid={appid}): request error\")\n",
    "                continue\n",
    "\n",
    "            # Parse only rows for target year\n",
    "            try:\n",
    "                parsed_rows = parse_year_data_from_html(html, target_year=year)\n",
    "            except Exception:\n",
    "                out_rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"rank\": rank,\n",
    "                    \"name\": name,\n",
    "                    \"appid\": appid,\n",
    "                    \"month\": None,\n",
    "                    \"avg_players\": None,\n",
    "                    \"peak_players\": None,\n",
    "                    \"status\": \"parse_error\",\n",
    "                })\n",
    "                print(f\"[{idx+1}/{total}] {name} (appid={appid}): parse error\")\n",
    "                continue\n",
    "\n",
    "            # no rows found for this yeaer \n",
    "            if not parsed_rows:\n",
    "                out_rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"rank\": rank,\n",
    "                    \"name\": name,\n",
    "                    \"appid\": appid,\n",
    "                    \"month\": None,\n",
    "                    \"avg_players\": None,\n",
    "                    \"peak_players\": None,\n",
    "                    \"status\": \"no_data_for_year\",\n",
    "                })\n",
    "                print(f\"[{idx+1}/{total}] {name} (appid={appid}): no data for year\")\n",
    "                continue\n",
    "\n",
    "            # Found rows: attach metadata \n",
    "            for pr in parsed_rows:\n",
    "                out_rows.append({\n",
    "                    \"year\": year,\n",
    "                    \"rank\": rank,\n",
    "                    \"name\": name,\n",
    "                    \"appid\": appid,\n",
    "                    \"month\": pr[\"month\"],\n",
    "                    \"avg_players\": pr[\"avg_players\"],\n",
    "                    \"peak_players\": pr[\"peak_players\"],\n",
    "                    \"status\": \"ok\",\n",
    "                })\n",
    "\n",
    "            print(f\"[{idx+1}/{total}] {name} ({appid}) -> ok ({len(parsed_rows)} months)\")\n",
    "\n",
    "    result_df = pd.DataFrame(\n",
    "        out_rows,\n",
    "        columns=[\n",
    "            \"year\", \n",
    "            \"rank\", \n",
    "            \"name\", \n",
    "            \"appid\",\n",
    "            \"month\", \n",
    "            \"avg_players\", \n",
    "            \"peak_players\",\n",
    "            \"status\",\n",
    "        ],)\n",
    "\n",
    "    # Sort for readability (rank, then month)\n",
    "    result_df = result_df.sort_values(\n",
    "                    by=[\"rank\", \"month\"], \n",
    "                    na_position=\"last\"\n",
    "                ).reset_index(drop=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def collect_year_range(\n",
    "    start_year: int,\n",
    "    end_year: int,\n",
    "    input_dir: Path,\n",
    "    input_pattern: str,      # e.g. \"{year}_top250_ids.csv\"\n",
    "    output_dir: Path,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    "    write_combined: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run collection across a year range using predictable filenames.\n",
    "\n",
    "    For each year:\n",
    "    - Build input file path from input_pattern\n",
    "    - Skip year if file missing\n",
    "    - Collect year data\n",
    "    - Write per-year CSV\n",
    "\n",
    "    Optionally:\n",
    "    - Combine all years into one DataFrame + CSV\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output_dir exists\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_parts = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "    #   build input_csv path from pattern\n",
    "        input_csv = input_dir / input_pattern.format(year=year)\n",
    "\n",
    "    #   if missing file: print skip and continue\n",
    "        if not input_csv.exists():\n",
    "            print(f\"{year} SKIP - missing input file:{input_csv}\")\n",
    "            continue\n",
    "\n",
    "        year_df = collect_one_year(\n",
    "            input_csv=input_csv,\n",
    "            year=year,\n",
    "            cache_dir=cache_dir,\n",
    "            use_cache=use_cache,\n",
    "            request_delay_sec=request_delay_sec,\n",
    "        )\n",
    "    #   write year_df to output_dir / f\"steamcharts_{year}_top250.csv\"\n",
    "        year_out_path = output_dir / f\"steamcharts_{year}_top250.csv\"\n",
    "        year_df.to_csv(year_out_path, index=False)\n",
    "        print(f\"[{year}] wrote {year_out_path} ({len(year_df)} rows)\")\n",
    "\n",
    "        status_counts = year_df[\"status\"].value_counts(dropna=False)\n",
    "        print(f\"[{year}] status summary:\\n{status_counts.to_string()}\")\n",
    "\n",
    "    #   append year_df to all_parts\n",
    "        all_parts.append(year_df)\n",
    "\n",
    "    # If nothing processed, return empty with expected schema\n",
    "    if not all_parts:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"year\", \"rank\", \"name\", \"appid\", \"month\",\n",
    "            \"avg_players\", \"peak_players\", \"status\"\n",
    "        ])\n",
    "\n",
    "    combined_df = pd.concat(all_parts, ignore_index=True)\n",
    "\n",
    "    if write_combined:\n",
    "        combined_out_path = output_dir / f\"steamcharts_{start_year}_{end_year}_combined.csv\"\n",
    "        combined_df.to_csv(combined_out_path, index=False)\n",
    "        print(f\"\\nWrote combined file: {combined_out_path} ({len(combined_df)} rows)\")\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status filtering\n",
    "Not all of the requests are expected to go through perfectly, so statuses are attatched to them so we can easily pick out which ones are missing data. \n",
    "\n",
    "This function takes a year, loads that year’s interim SteamCharts CSV, filters to rows where `status == \"ok\"`, and writes the filtered result to `data/02-processed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def keep_only_ok_status(\n",
    "    year: int,\n",
    "    interim_dir: Path = Path(\"data/01-interim\"),\n",
    "    processed_dir: Path = Path(\"data/02-processed\"),\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Reads one yearly interim SteamCharts CSV and writes an ok-only version.\n",
    "\n",
    "    Input:\n",
    "      data/01-interim/steamcharts_{year}_top250.csv\n",
    "    Output:\n",
    "      data/02-processed/steamcharts_{year}_top250_ok.csv\n",
    "\n",
    "    Returns the output path.\n",
    "    \"\"\"\n",
    "    input_path = interim_dir / f\"steamcharts_{year}_top250.csv\"\n",
    "    output_path = processed_dir / f\"steamcharts_{year}_top250_ok.csv\"\n",
    "\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    if \"status\" not in df.columns:\n",
    "        raise ValueError(f\"'status' column not found in {input_path}\")\n",
    "\n",
    "    df_ok = df[df[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_ok.to_csv(output_path, index=False)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "\n",
    "def make_combined_ok_file(\n",
    "    start_year: int,\n",
    "    end_year: int,\n",
    "    processed_dir: Path = Path(\"data/02-processed\"),\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Combines the yearly ok-only files into one ok-only combined file.\n",
    "\n",
    "    Inputs:\n",
    "      data/02-processed/steamcharts_{year}_top250_ok.csv\n",
    "    Output:\n",
    "      data/02-processed/steamcharts_{start}_{end}_ok.csv\n",
    "\n",
    "    Returns the output path.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        fp = processed_dir / f\"steamcharts_{year}_top250_ok.csv\"\n",
    "        if fp.exists():\n",
    "            parts.append(pd.read_csv(fp))\n",
    "\n",
    "    if not parts:\n",
    "        raise FileNotFoundError(\"No yearly ok-only files found to combine.\")\n",
    "\n",
    "    combined_ok = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    out_path = processed_dir / f\"steamcharts_{start_year}_{end_year}_ok.csv\"\n",
    "    combined_ok.to_csv(out_path, index=False)\n",
    "\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Data Quality and Summary\n",
    "This section loads the combined SteamCharts output and reports high-level checks used before modeling:\n",
    "- available columns,\n",
    "- dataset size,\n",
    "- status counts from collection,\n",
    "- period-level descriptive summaries for `avg_players` and `peak_players` using only `status == \"ok\"` rows.\n",
    "These checks provide a compact overview of data completeness and trend direction across pre-COVID, COVID, and post-COVID periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_steamcharts_summary(csv_path: Path, label: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Print a compact data-quality + descriptive summary for a SteamCharts scrape CSV.\n",
    "\n",
    "    Reports:\n",
    "    - Columns\n",
    "    - Overall size\n",
    "    - Status counts (if column exists)\n",
    "    - Period-level summary of avg_players / peak_players\n",
    "      (uses status=='ok' rows when status column exists)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    header = f\"=== {label} ===\" if label else \"===\"\n",
    "    print(f\"\\n{header}\")\n",
    "    print(f\"File: {csv_path}\")\n",
    "    print(f\"Rows: {df.shape[0]} | Columns: {df.shape[1]}\")\n",
    "\n",
    "    print(\"\\nColumns:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Status counts (only if present)\n",
    "    if \"status\" in df.columns:\n",
    "        print(\"\\nStatus counts:\")\n",
    "        print(df[\"status\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "    # Period-level summary (descriptive only)\n",
    "    required = {\"year\", \"avg_players\", \"peak_players\"}\n",
    "    if required.issubset(df.columns):\n",
    "        tmp = df.copy()\n",
    "\n",
    "        # light coercion for summary calculations only\n",
    "        tmp[\"year\"] = pd.to_numeric(tmp[\"year\"], errors=\"coerce\")\n",
    "        tmp[\"avg_players\"] = pd.to_numeric(tmp[\"avg_players\"], errors=\"coerce\")\n",
    "        tmp[\"peak_players\"] = pd.to_numeric(tmp[\"peak_players\"], errors=\"coerce\")\n",
    "\n",
    "        # If status exists, restrict to ok for summaries\n",
    "        if \"status\" in tmp.columns:\n",
    "            tmp = tmp[tmp[\"status\"] == \"ok\"].copy()\n",
    "\n",
    "        def period_label(y):\n",
    "            if pd.isna(y):\n",
    "                return \"unknown\"\n",
    "            y = int(y)\n",
    "            if 2018 <= y <= 2019:\n",
    "                return \"pre_covid\"\n",
    "            if 2020 <= y <= 2021:\n",
    "                return \"covid\"\n",
    "            if 2022 <= y <= 2023:\n",
    "                return \"post_covid\"\n",
    "            return \"other\"\n",
    "\n",
    "        tmp[\"period\"] = tmp[\"year\"].apply(period_label)\n",
    "\n",
    "        period_summary = (\n",
    "            tmp.groupby(\"period\", as_index=False)\n",
    "               .agg(\n",
    "                   n_rows=(\"period\", \"size\"),\n",
    "                   avg_players_mean=(\"avg_players\", \"mean\"),\n",
    "                   avg_players_median=(\"avg_players\", \"median\"),\n",
    "                   peak_players_mean=(\"peak_players\", \"mean\"),\n",
    "                   peak_players_median=(\"peak_players\", \"median\"),\n",
    "               )\n",
    "               .sort_values(\"period\")\n",
    "        )\n",
    "\n",
    "        print(\"\\nPeriod-level summary (avg/peak players):\")\n",
    "        print(period_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Run Configuration\n",
    "`main()` centralizes the user-editable run settings (year range, input pattern, output directory, cache directory, and delay settings).  \n",
    "This keeps the rest of the pipeline stable while making it easy to rerun collection with different folders or year ranges.  \n",
    "The run writes one CSV per year and a combined CSV for downstream summary and analysis, then prints a summary of the shape of the data, tidys the data, then prints the summary once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_year = 2018\n",
    "    end_year = 2023\n",
    "\n",
    "    input_dir = Path(\"data/02-processed\")\n",
    "    input_pattern = \"{year}_top250_ids.csv\"\n",
    "    output_dir = Path(\"data/01-interim\")\n",
    "    cache_dir = Path(\"data/00-raw/steamcharts_cache\")\n",
    "\n",
    "    # Run scraper to create interim per-year + combined\n",
    "    collect_year_range(\n",
    "        start_year=start_year,\n",
    "        end_year=end_year,\n",
    "        input_dir=input_dir,\n",
    "        input_pattern=input_pattern,\n",
    "        output_dir=output_dir,\n",
    "        cache_dir=cache_dir,\n",
    "        use_cache=True,\n",
    "        request_delay_sec=0.6,\n",
    "        write_combined=True,\n",
    "    )\n",
    "\n",
    "    # Print BEFORE summary (combined interim)\n",
    "    combined_before = output_dir / f\"steamcharts_{start_year}_{end_year}_combined.csv\"\n",
    "    report_steamcharts_summary(combined_before, label=\"Combined BEFORE status filtering\")\n",
    "\n",
    "    # Create yearly ok-only files\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        keep_only_ok_status(year, interim_dir=output_dir, processed_dir=Path(\"data/02-processed\"))\n",
    "\n",
    "    # Create combined ok-only file\n",
    "    combined_after = make_combined_ok_file(start_year, end_year, processed_dir=Path(\"data/02-processed\"))\n",
    "\n",
    "    # Print AFTER summary (combined ok-only)\n",
    "    report_steamcharts_summary(combined_after, label=\"Combined AFTER status filtering (ok-only)\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Collection\n",
    "  [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "> Our project does not involve human subjects directly. We are analyzing publicly avaliable gaming statistics from Steam, which does not require consent.\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "> We agree that by focusing solely on Steam data, we introduce platform bias. However, our research question is specifically aimed on PC gaming on Steam.\n",
    "\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "> Our dataset contains no personally identifiable information (PII). We are using aggregate player statistics and game mode data only.\n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "> We are not collecting data on protected groups such as gender or race, as our analysis focuses on gaming trends at the aggregate game mode rather than individual player demographics. \n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "> The data we are using is publicly available from Steam Charts and SteamDB. We are not collecting any new data or storing sensitive information.\n",
    "\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    "> Not applicable, as we are not collecting or storing any personal information from indivisuals.\n",
    "\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "> Since we are using publicly available data and not collecting new data, data retention is not our concern. Any data we download for analysis will be retained only for the duration of this project.\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "> We acknowledge that other gaming platforms exist (e.g., Nintendo, Sony Playstation, Microsoft Xbox, Mobile) which provide different gaming experiences. Our dataset is limited to Steam users, who may not be representative of the global gaming population (e.g., mobile gamers or console game players). However, our research question is intentionally narrowed to PC gaming on Steam due to data availability and accessibility. \n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "> There could be potential bias in our dataset if certain games have wealthy backers who promote them more heavily, which could inflate their popularity metrics. We are aware of this possibility when interpreting our results.\n",
    "\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    "> We are committed to representing our data honestly and will strive to create visualization and statistics that accurately reflect the underlying trends without misleading interpretations.\n",
    "\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    "> Privacy is not a conter for our analysis since we are not using any data with PII. \n",
    "\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "> We are committed to documenting our analysis process thoroughly to ensure reproducibility. This includes maintaining clear records of our data sources, processing steps.\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "> We are considering PC players as a general population. Since our analysis focuses on game mode trends rather than player demographics, bias and discrimination concerns are not relevant to our research.\n",
    "\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    "> Not applicable for the same reason as D.1.\n",
    "\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    "> We are not creating a predictive model or optimizing for specific metrics.\n",
    "\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    "> Not applicable, as we are not buliding a predictive model or making automated decisions.\n",
    "\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "> While we are not buliding a model, we will clearly communicate the limitation of our analysis, including our focus on Steam data only and potential biases in the dataset.\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    "> Not applicable. We are not deploying a model. This is a research project analyzing game mode trends.\n",
    "\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    "> Not applicable as our analysis is unlikely to cause harm and we are not deploying a system that impacts indivisuals.\n",
    "\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    "> Not applicable for the Same reason as E.1\n",
    "\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
    "> We are using data for research, but game developers might usage our results. For example, if they see old games were popular, they might just copy them instead of making new, creative games. This could lead to too many similar games and less variety for players."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team expectations are also [found separately here](./admin/rules.md).\n",
    "\n",
    "### Communication\n",
    "**Discord** is our main form of communication. We have a group chat.\n",
    "- **Responding** If there is a message that needs responding to/acknowledgement, respond within about 1 day/24 hours (48 hours can be acceptable if there's an emergency). The exception to this if we have a planned meeting coming up and we ask for a ready check, then it's expected that there's an almost immediate response.\n",
    "- **Respect** Stay reasonably respectful to one another. It's okay to disagree, but do talk about the issue together or bring in another person (or the entire group) to discuss the matter if needed to mediate. If you don't talk about something, there's no way we'd know what's wrong.\n",
    "  \n",
    "### Missing Tasks/Meetings\n",
    "- **Tasks** If you can't complete a task, let us know as soon as possible (i.e. as soon as you find out) so we can reorganize task assignment or move our schedule around.\n",
    "- **Meetings** If you can't make a meeting, that's okay, and it's not detrimental. However, that would mean you can't provide your input on something live. You can share your thoughts and ideas in our group chat in this event so we can discuss your ideas. We do take meeting notes, so please read them to stay up to date with the team.\n",
    "\n",
    "### Team Structure and Decision Making\n",
    "- **Team Roles** We don't plan on having established team roles, but we'll try to have everyone do a bit of everything (to the best of our ability). The only real \"role\" we'll have is one note taker per meeting.\n",
    "- **Task Tracking** We'll use the GitHub Projects tab/Kanban on the team repository. \n",
    "- **Decision Making** If it comes to a decision, we'll have a vote to decide (more votes = win).\n",
    "\n",
    "### Addressing Problem Members\n",
    "This is our protocol on addressing non-responsive teammates/those refusing to do work:\n",
    "1. First offense: check-in and see if everything is okay.\n",
    "2. Second offense: what we do depends, but we'll talk with you again.\n",
    "3. Clearly becoming a pattern: talk to a TA and/or the professor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Type | Date | Meeting/Due Time | To Complete Before Meeting | Discuss at Meeting |\n",
    "| ---- | ---- | ---- | ---- |  ---- |\n",
    "| Meeting | 2/22 | 2pm  | Read up on EDA checkpoint requirements; come into meeting with ideas on how to approach things | Discuss EDA and split up tasks for EDA checkpoint. |\n",
    "| Meeting | 3/1  | 2pm  | Make 70-80% progress on EDA tasks | Check in on EDA progress and see what needs to be done. |\n",
    "| **DUE** EDA Checkpoint | 3/4 | 11:59pm | - | - |\n",
    "| Meeting | 3/8  | 2pm  | Wrap up any loose ends we didn't finish (if applicable). Read up on final project expectations. | Discuss final project tasks and split up tasks. |\n",
    "| Meeting | 3/15 | 2pm  | Make about 80% progress on final project tasks. | Discuss the video work. |\n",
    "| **DUE** Final Project + Video | 3/18 | 11:59pm | - | - |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COGS108_FA25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
