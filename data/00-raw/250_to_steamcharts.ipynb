{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53708734",
   "metadata": {},
   "source": [
    "# Collecting (month, average players, peak players) per game per month each year\n",
    "Takes in an array of arrays that looks like: [ [rank, app_id], [rank, app_id], ... ]\n",
    "Outputs: [ [app_id, month, avg_players, peak_players], ... ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e031862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and globals\n",
    "import csv\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path \n",
    "from typing import List, Tuple, Union, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # for parsing HTML docs\n",
    "\n",
    "# Base URL pattern for SteamCharts game pages\n",
    "# inject app_id into {app_id}, e.g. app_id=730 -> https://steamcharts.com/app/730\n",
    "BASE_URL = \"https://steamcharts.com/app/{appid}\"\n",
    "\n",
    "# Apprently some sites block requests that do not provide a browser-like user agent.\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; SteamChartsYearScraper/1.0)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeee358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First fetch length: 50375\n",
      "Second fetch length: 50375\n",
      "Same content: True\n",
      "Cache files: 1\n"
     ]
    }
   ],
   "source": [
    "# Some utlity helpers! ==========================================================================\n",
    "\n",
    "def clean_num(value: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Convert numeric text to float.\n",
    "\n",
    "    Examples:\n",
    "    - \"12,345.6\" -> 12345.6\n",
    "    - \"-\" or \"\"  -> None\n",
    "\n",
    "    Why this exists:\n",
    "    - SteamCharts numbers include commas and sometimes placeholders.\n",
    "    - Downstream analysis is easier if values are numeric.\n",
    "    \"\"\"\n",
    "    # TODO: strip whitespace, remove commas, handle blanks/dashes, return float or None\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def build_game_url(appid: int) -> str:\n",
    "    \"\"\"\n",
    "    Build SteamCharts URL for one appid.\n",
    "    Example: appid=730 -> \"https://steamcharts.com/app/730\"\n",
    "    \"\"\"\n",
    "    return BASE_URL.format(appid=int(appid))\n",
    "\n",
    "\n",
    "\n",
    "def cache_key_for_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Build deterministic cache filename key from URL.\n",
    "    Using md5 keeps filenames short and filesystem-safe.\n",
    "    Why this exists:\n",
    "    - URL text may not be ideal as a filename.\n",
    "    - Hash gives stable and filesystem-safe names.\n",
    "    \"\"\"\n",
    "    return hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "# Input and loading validation ==================================================================\n",
    "\n",
    "\n",
    "def load_input_csv(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and validate one input CSV.\n",
    "    (Probably not neccessary, but I think it's good practice just in case)\n",
    "\n",
    "    Required columns:\n",
    "    - rank\n",
    "    - name\n",
    "    - appid\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned DataFrame with normalized dtypes:\n",
    "      rank:int, name:str, appid:int\n",
    "\n",
    "    Why this exists:\n",
    "    - Fail fast with clear errors if schema is wrong.\n",
    "    - Avoid repeated type conversions elsewhere.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # Read CSV (consider encoding=\"utf-8-sig\")\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Validate required columns\n",
    "    required = {\"rank\", \"name\", \"appid\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"{file_path.name} is missing required columns: {sorted(missing)}. \"\n",
    "            f\"Found columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Keep only needed columns in predictable order \n",
    "    df = df[[\"rank\", \"name\", \"appid\"]].copy()\n",
    "\n",
    "    # Convert numeric fields and fail LOUDLY if invalid\n",
    "    df[\"rank\"] = pd.to_numeric(df[\"rank\"], errors=\"raise\").astype(int)\n",
    "    df[\"appid\"] = pd.to_numeric(df[\"appid\"], errors=\"raise\").astype(int)\n",
    "\n",
    "    # 4) Strip whitespace on name\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.strip()\n",
    "\n",
    "    # Remove rows with empty names \n",
    "    df = df[df[\"name\"] != \"\"].copy()\n",
    "\n",
    "    # drop duplicates on rank+appid\n",
    "    df = df.drop_duplicates(subset=[\"rank\", \"appid\"]).reset_index(drop=True)\n",
    "\n",
    "    # Return cleaned DataFrame\n",
    "    # Sort by rank for deterministic processing\n",
    "    df = df.sort_values(\"rank\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Network + cache =================================================================================\n",
    "\n",
    "\n",
    "def get_game_page_html(\n",
    "    appid: int,\n",
    "    session: requests.Session,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return HTML for one game page, using cache when available.\n",
    "\n",
    "    Flow:\n",
    "    1) Build game URL from appid\n",
    "    2) Compute cache filename from URL hash\n",
    "    3) If cache exists and use_cache=True -> return cached HTML\n",
    "    4) Else fetch from network, save cache, sleep briefly, return HTML\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Ensure cache_dir exists\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 2) Compute URL and cache filename\n",
    "    url = build_game_url(appid)\n",
    "    cache_file = cache_dir / f\"{cache_key_for_url(url)}.html\"\n",
    "\n",
    "    # 3) If cache hit and use_cache: read + return\n",
    "    if cache_file.exists() and use_cache:\n",
    "        return cache_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Live request path\n",
    "    resp = session.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    html = resp.text\n",
    "\n",
    "    # Save to cache for future runs\n",
    "    cache_file.write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "    # Don't attack our lord and savior GabeN with rapid-fire requests\n",
    "    time.sleep(request_delay_sec)\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "# HTML parsing ===================================================================================\n",
    "\n",
    "\n",
    "def parse_year_data_from_html(html: str, target_year: int) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parse SteamCharts monthly table from one app page, filtered to target year.\n",
    "\n",
    "    Input:\n",
    "    - html: raw page HTML\n",
    "    - target_year: year to keep (e.g., 2021)\n",
    "\n",
    "    Output row shape:\n",
    "    {\n",
    "      \"month\": \"YYYY-MM\",\n",
    "      \"avg_players\": float|None,\n",
    "      \"peak_players\": float|None\n",
    "    }\n",
    "\n",
    "    Why this exists:\n",
    "    - Pure parser function (HTML in -> structured rows out).\n",
    "    - Easy to test independently from I/O.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Parse html with BeautifulSoup\n",
    "    # 2) Select monthly table rows (table.common-table tbody tr)\n",
    "    # 3) For each row, parse month/avg/peak\n",
    "    # 4) Skip \"Last 30 Days\"\n",
    "    # 5) Parse month text with pd.to_datetime(..., format=\"%B %Y\")\n",
    "    # 6) Keep rows where parsed year == target_year\n",
    "    # 7) Clean numeric fields with clean_num()\n",
    "    # 8) Return rows sorted by month asc\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# year collection ====================================================================================\n",
    "\n",
    "\n",
    "def collect_one_year(\n",
    "    input_csv: Path,\n",
    "    year: int,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect SteamCharts data for one year's input list.\n",
    "\n",
    "    Steps:\n",
    "    - Load CSV (rank, name, appid)\n",
    "    - For each game:\n",
    "      - Fetch or read cached HTML\n",
    "      - Parse target-year monthly rows\n",
    "      - Emit result rows with status labels\n",
    "\n",
    "    Status values:\n",
    "    - \"ok\"               : parsed monthly rows exist\n",
    "    - \"no_data_for_year\" : page loaded, but no rows for that year\n",
    "    - \"request_error\"    : failed HTTP request\n",
    "    - \"parse_error\"      : page fetched but parse failed\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) games_df = load_input_csv(input_csv)\n",
    "    # 2) Initialize out_rows = []\n",
    "    # 3) Create requests.Session()\n",
    "    # 4) Loop games_df rows:\n",
    "    #    a) read rank, name, appid\n",
    "    #    b) try get_game_page_html(...)\n",
    "    #       - on failure append one request_error row, continue\n",
    "    #    c) try parse_year_data_from_html(...)\n",
    "    #       - on failure append one parse_error row, continue\n",
    "    #    d) if parsed rows empty:\n",
    "    #          append one no_data_for_year row\n",
    "    #       else:\n",
    "    #          append one row per month with status=\"ok\"\n",
    "    # 5) Build DataFrame with fixed column order\n",
    "    # 6) Return DataFrame\n",
    "    pass\n",
    "\n",
    "\n",
    "def collect_year_range(\n",
    "    start_year: int,\n",
    "    end_year: int,\n",
    "    input_dir: Path,\n",
    "    input_pattern: str,      # e.g. \"{year}_top250_ids.csv\"\n",
    "    output_dir: Path,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    "    write_combined: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run collection across a year range using predictable filenames.\n",
    "\n",
    "    Example:\n",
    "    - input_pattern \"{year}_top250_ids.csv\"\n",
    "    - for year=2018 -> \"2018_top250_ids.csv\"\n",
    "\n",
    "    Output:\n",
    "    - Writes one CSV per year to output_dir\n",
    "    - Optionally writes one combined CSV\n",
    "    - Returns combined DataFrame (or empty DataFrame if nothing processed)\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Ensure output_dir exists\n",
    "    # 2) all_parts = []\n",
    "    # 3) for year in range(start_year, end_year + 1):\n",
    "    #    a) build input_csv path from pattern\n",
    "    #    b) if missing file: print skip and continue\n",
    "    #    c) year_df = collect_one_year(...)\n",
    "    #    d) write year_df to output_dir / f\"steamcharts_{year}_top250.csv\"\n",
    "    #    e) append year_df to all_parts\n",
    "    # 4) if all_parts empty: return empty DataFrame with expected columns\n",
    "    # 5) combined_df = concat all_parts\n",
    "    # 6) if write_combined: save combined CSV\n",
    "    # 7) return combined_df\n",
    "    pass\n",
    "\n",
    "# TESTING ===================================================================================================\n",
    "def _test_load_input_csv():\n",
    "    test_path = Path(\"2018_top250_ids.csv\")\n",
    "    df = load_input_csv(test_path)\n",
    "\n",
    "    print(\"Loaded rows:\", len(df))\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(df.head(10).to_string(index=False))\n",
    "\n",
    "def _test_parse_year_data_from_html():\n",
    "    cache_dir = Path(\"steamcharts_cache_test\")\n",
    "    appid = 730  # CS\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        # First call should fetch live and cache\n",
    "        html1 = get_game_page_html(\n",
    "            appid=appid,\n",
    "            session=session,\n",
    "            cache_dir=cache_dir,\n",
    "            use_cache=True,\n",
    "            request_delay_sec=0.6,\n",
    "        )\n",
    "        print(\"First fetch length:\", len(html1))\n",
    "\n",
    "        # Second call should come from cache (no live request needed)\n",
    "        html2 = get_game_page_html(\n",
    "            appid=appid,\n",
    "            session=session,\n",
    "            cache_dir=cache_dir,\n",
    "            use_cache=True,\n",
    "            request_delay_sec=0.6,\n",
    "        )\n",
    "        print(\"Second fetch length:\", len(html2))\n",
    "\n",
    "    print(\"Same content:\", html1 == html2)\n",
    "    print(\"Cache files:\", len(list(cache_dir.glob(\"*.html\"))))\n",
    "\n",
    "# MAIN =======================================================================================================\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Central run configuration.\n",
    "\n",
    "    Keep all user-editable settings here so the rest of the code\n",
    "    stays stable and easy to reason about.\n",
    "    \"\"\"\n",
    "    # TODO: configure these\n",
    "    start_year = 2018\n",
    "    end_year = 2023\n",
    "    input_dir = Path(\".\")\n",
    "    input_pattern = \"{year}_top250_ids.csv\"\n",
    "    output_dir = Path(\"outputs\")\n",
    "    cache_dir = Path(\"steamcharts_cache\")\n",
    "\n",
    "    # TODO: call collect_year_range(...)\n",
    "    # TODO: print small preview (head) and summary counts\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _test_parse_year_data_from_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COGS108_FA25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
