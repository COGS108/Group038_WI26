{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53708734",
   "metadata": {},
   "source": [
    "# Collecting (month, average players, peak players) per game per month each year\n",
    "Takes in an array of arrays that looks like: [ [rank, app_id], [rank, app_id], ... ]\n",
    "Outputs: [ [app_id, month, avg_players, peak_players], ... ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and globals\n",
    "import csv\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path \n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # for parsing HTML docs\n",
    "\n",
    "# Base URL pattern for SteamCharts game pages\n",
    "# inject app_id into {app_id}, e.g. app_id=730 -> https://steamcharts.com/app/730\n",
    "BASE_URL = \"https://steamcharts.com/app/{appid}\"\n",
    "\n",
    "# Apprently some sites block requests that do not provide a browser-like user agent.\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; SteamChartsYearScraper/1.0)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeee358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed rows for appid=730, year=2021: 12\n",
      "Preview:\n",
      "{'month': '2021-01', 'avg_players': 743209.66, 'peak_players': 1124553.0}\n",
      "{'month': '2021-02', 'avg_players': 741013.24, 'peak_players': 1123485.0}\n",
      "{'month': '2021-03', 'avg_players': 740927.82, 'peak_players': 1198581.0}\n",
      "{'month': '2021-04', 'avg_players': 723346.52, 'peak_players': 1148077.0}\n",
      "{'month': '2021-05', 'avg_players': 659888.89, 'peak_players': 1087197.0}\n",
      "{'month': '2021-06', 'avg_players': 549347.08, 'peak_players': 929940.0}\n",
      "{'month': '2021-07', 'avg_players': 506067.36, 'peak_players': 763523.0}\n",
      "{'month': '2021-08', 'avg_players': 512081.96, 'peak_players': 802544.0}\n",
      "{'month': '2021-09', 'avg_players': 512350.92, 'peak_players': 942519.0}\n",
      "{'month': '2021-10', 'avg_players': 512435.85, 'peak_players': 864966.0}\n",
      "{'month': '2021-11', 'avg_players': 548161.67, 'peak_players': 935593.0}\n",
      "{'month': '2021-12', 'avg_players': 546614.19, 'peak_players': 950586.0}\n"
     ]
    }
   ],
   "source": [
    "# Some utlity helpers! ==========================================================================\n",
    "\n",
    "def clean_num(value: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Convert numeric text to float.\n",
    "\n",
    "    Handles:\n",
    "    - commas: \"12,345.6\" -> 12345.6\n",
    "    - blanks/dashes -> None\n",
    "    - invalid values -> None\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "\n",
    "    text = str(value).strip().replace(\",\", \"\")\n",
    "    if text in {\"\", \"-\", \"â€”\"}:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_game_url(appid: int) -> str:\n",
    "    \"\"\"\n",
    "    Build SteamCharts URL for one appid.\n",
    "    Example: appid=730 -> \"https://steamcharts.com/app/730\"\n",
    "    \"\"\"\n",
    "    return BASE_URL.format(appid=int(appid))\n",
    "\n",
    "\n",
    "\n",
    "def cache_key_for_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Build deterministic cache filename key from URL.\n",
    "    Using md5 keeps filenames short and filesystem-safe.\n",
    "    Why this exists:\n",
    "    - URL text may not be ideal as a filename.\n",
    "    - Hash gives stable and filesystem-safe names.\n",
    "    \"\"\"\n",
    "    return hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "# Input and loading validation ==================================================================\n",
    "\n",
    "\n",
    "def load_input_csv(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and validate one input CSV.\n",
    "    (Probably not neccessary, but I think it's good practice just in case)\n",
    "\n",
    "    Required columns:\n",
    "    - rank\n",
    "    - name\n",
    "    - appid\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned DataFrame with normalized dtypes:\n",
    "      rank:int, name:str, appid:int\n",
    "\n",
    "    Why this exists:\n",
    "    - Fail fast with clear errors if schema is wrong.\n",
    "    - Avoid repeated type conversions elsewhere.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # Read CSV (consider encoding=\"utf-8-sig\")\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Validate required columns\n",
    "    required = {\"rank\", \"name\", \"appid\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"{file_path.name} is missing required columns: {sorted(missing)}. \"\n",
    "            f\"Found columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Keep only needed columns in predictable order \n",
    "    df = df[[\"rank\", \"name\", \"appid\"]].copy()\n",
    "\n",
    "    # Convert numeric fields and fail LOUDLY if invalid\n",
    "    df[\"rank\"] = pd.to_numeric(df[\"rank\"], errors=\"raise\").astype(int)\n",
    "    df[\"appid\"] = pd.to_numeric(df[\"appid\"], errors=\"raise\").astype(int)\n",
    "\n",
    "    # 4) Strip whitespace on name\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.strip()\n",
    "\n",
    "    # Remove rows with empty names \n",
    "    df = df[df[\"name\"] != \"\"].copy()\n",
    "\n",
    "    # drop duplicates on rank+appid\n",
    "    df = df.drop_duplicates(subset=[\"rank\", \"appid\"]).reset_index(drop=True)\n",
    "\n",
    "    # Return cleaned DataFrame\n",
    "    # Sort by rank for deterministic processing\n",
    "    df = df.sort_values(\"rank\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Network + cache =================================================================================\n",
    "\n",
    "\n",
    "def get_game_page_html(\n",
    "    appid: int,\n",
    "    session: requests.Session,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return HTML for one game page, using cache when available.\n",
    "\n",
    "    Flow:\n",
    "    1) Build game URL from appid\n",
    "    2) Compute cache filename from URL hash\n",
    "    3) If cache exists and use_cache=True -> return cached HTML\n",
    "    4) Else fetch from network, save cache, sleep briefly, return HTML\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Ensure cache_dir exists\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 2) Compute URL and cache filename\n",
    "    url = build_game_url(appid)\n",
    "    cache_file = cache_dir / f\"{cache_key_for_url(url)}.html\"\n",
    "\n",
    "    # 3) If cache hit and use_cache: read + return\n",
    "    if cache_file.exists() and use_cache:\n",
    "        return cache_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Live request path\n",
    "    resp = session.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    html = resp.text\n",
    "\n",
    "    # Save to cache for future runs\n",
    "    cache_file.write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "    # Don't attack our lord and savior GabeN with rapid-fire requests\n",
    "    time.sleep(request_delay_sec)\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "# HTML parsing ===================================================================================\n",
    "\n",
    "\n",
    "def parse_year_data_from_html(html: str, target_year: int) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parse SteamCharts monthly table from one app page, filtered to target year.\n",
    "\n",
    "    Input:\n",
    "    - html: raw page HTML\n",
    "    - target_year: year to keep (e.g., 2021)\n",
    "\n",
    "    Output row shape:\n",
    "    {\n",
    "      \"month\": \"YYYY-MM\",\n",
    "      \"avg_players\": float|None,\n",
    "      \"peak_players\": float|None\n",
    "    }\n",
    "\n",
    "    Why this exists:\n",
    "    - Pure parser function (HTML in -> structured rows out).\n",
    "    - Easy to test independently from I/O.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Parse html with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    parsed_rows = []\n",
    "\n",
    "    # 2) Select monthly table rows (table.common-table tbody tr)\n",
    "    rows = soup.select(\"table.common-table tbody tr\")\n",
    "\n",
    "    # 3) For each row, parse month/avg/peak\n",
    "    for tr in rows: \n",
    "        tds = tr.find_all(\"td\")\n",
    "\n",
    "        # Monthly rows should have at least 5 columns\n",
    "        # Month | Avg. Players | Gain | Gain % | Peak Players\n",
    "        if len(tds) < 5:\n",
    "            continue\n",
    "\n",
    "        month_text = tds[0].get_text(\" \", strip=True)\n",
    "\n",
    "        # 4) Skip \"Last 30 Days\"\n",
    "        if month_text.lower() == \"last 30 days\":\n",
    "            continue\n",
    "\n",
    "        # 5) Parse month text with pd.to_datetime(..., format=\"%B %Y\")\n",
    "        month_dt = pd.to_datetime(month_text, format=\"%B %Y\", errors=\"coerce\")\n",
    "        if pd.isna(month_dt):\n",
    "            continue\n",
    "\n",
    "        # 6) Keep rows where parsed year == target_year\n",
    "        if int(month_dt.year) != int(target_year):\n",
    "            continue\n",
    "\n",
    "        # 7) Clean numeric fields with clean_num()\n",
    "        avg_text = tds[1].get_text(\" \", strip=True)\n",
    "        peak_text = tds[4].get_text(\" \", strip=True)\n",
    "\n",
    "        # 8) Return rows sorted by month asc\n",
    "        parsed_rows.append({\n",
    "            \"month\": month_dt.strftime(\"%Y-%m\"),\n",
    "            \"avg_players\": clean_num(avg_text),\n",
    "            \"peak_players\": clean_num(peak_text),\n",
    "        })\n",
    "\n",
    "    # Keep output in chronological order\n",
    "    parsed_rows.sort(key=lambda r: r[\"month\"])\n",
    "    return parsed_rows\n",
    "\n",
    "\n",
    "\n",
    "# year collection ====================================================================================\n",
    "\n",
    "\n",
    "def collect_one_year(\n",
    "    input_csv: Path,\n",
    "    year: int,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect SteamCharts data for one year's input list.\n",
    "\n",
    "    Steps:\n",
    "    - Load CSV (rank, name, appid)\n",
    "    - For each game:\n",
    "      - Fetch or read cached HTML\n",
    "      - Parse target-year monthly rows\n",
    "      - Emit result rows with status labels\n",
    "\n",
    "    Status values:\n",
    "    - \"ok\"               : parsed monthly rows exist\n",
    "    - \"no_data_for_year\" : page loaded, but no rows for that year\n",
    "    - \"request_error\"    : failed HTTP request\n",
    "    - \"parse_error\"      : page fetched but parse failed\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) games_df = load_input_csv(input_csv)\n",
    "    # 2) Initialize out_rows = []\n",
    "    # 3) Create requests.Session()\n",
    "    # 4) Loop games_df rows:\n",
    "    #    a) read rank, name, appid\n",
    "    #    b) try get_game_page_html(...)\n",
    "    #       - on failure append one request_error row, continue\n",
    "    #    c) try parse_year_data_from_html(...)\n",
    "    #       - on failure append one parse_error row, continue\n",
    "    #    d) if parsed rows empty:\n",
    "    #          append one no_data_for_year row\n",
    "    #       else:\n",
    "    #          append one row per month with status=\"ok\"\n",
    "    # 5) Build DataFrame with fixed column order\n",
    "    # 6) Return DataFrame\n",
    "    pass\n",
    "\n",
    "\n",
    "def collect_year_range(\n",
    "    start_year: int,\n",
    "    end_year: int,\n",
    "    input_dir: Path,\n",
    "    input_pattern: str,      # e.g. \"{year}_top250_ids.csv\"\n",
    "    output_dir: Path,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    "    write_combined: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run collection across a year range using predictable filenames.\n",
    "\n",
    "    Example:\n",
    "    - input_pattern \"{year}_top250_ids.csv\"\n",
    "    - for year=2018 -> \"2018_top250_ids.csv\"\n",
    "\n",
    "    Output:\n",
    "    - Writes one CSV per year to output_dir\n",
    "    - Optionally writes one combined CSV\n",
    "    - Returns combined DataFrame (or empty DataFrame if nothing processed)\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Ensure output_dir exists\n",
    "    # 2) all_parts = []\n",
    "    # 3) for year in range(start_year, end_year + 1):\n",
    "    #    a) build input_csv path from pattern\n",
    "    #    b) if missing file: print skip and continue\n",
    "    #    c) year_df = collect_one_year(...)\n",
    "    #    d) write year_df to output_dir / f\"steamcharts_{year}_top250.csv\"\n",
    "    #    e) append year_df to all_parts\n",
    "    # 4) if all_parts empty: return empty DataFrame with expected columns\n",
    "    # 5) combined_df = concat all_parts\n",
    "    # 6) if write_combined: save combined CSV\n",
    "    # 7) return combined_df\n",
    "    pass\n",
    "\n",
    "# TESTING ===================================================================================================\n",
    "def _test_load_input_csv():\n",
    "    test_path = Path(\"2018_top250_ids.csv\")\n",
    "    df = load_input_csv(test_path)\n",
    "\n",
    "    print(\"Loaded rows:\", len(df))\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(df.head(10).to_string(index=False))\n",
    "\n",
    "\n",
    "def _test_fetch_html():\n",
    "    cache_dir = Path(\"steamcharts_cache_test\")\n",
    "    appid = 730  # CS\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        # First call should fetch live and cache\n",
    "        html1 = get_game_page_html(\n",
    "            appid=appid,\n",
    "            session=session,\n",
    "            cache_dir=cache_dir,\n",
    "            use_cache=True,\n",
    "            request_delay_sec=0.6,\n",
    "        )\n",
    "        print(\"First fetch length:\", len(html1))\n",
    "\n",
    "        # Second call should come from cache (no live request needed)\n",
    "        html2 = get_game_page_html(\n",
    "            appid=appid,\n",
    "            session=session,\n",
    "            cache_dir=cache_dir,\n",
    "            use_cache=True,\n",
    "            request_delay_sec=0.6,\n",
    "        )\n",
    "        print(\"Second fetch length:\", len(html2))\n",
    "\n",
    "    print(\"Same content:\", html1 == html2)\n",
    "    print(\"Cache files:\", len(list(cache_dir.glob(\"*.html\"))))\n",
    "\n",
    "\n",
    "def _test_parse_html():\n",
    "    appid = 730\n",
    "    target_year = 2021\n",
    "    cache_dir = Path(\"steamcharts_cache_test\")\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        html = get_game_page_html(\n",
    "            appid=appid,\n",
    "            session=session,\n",
    "            cache_dir=cache_dir,\n",
    "            use_cache=True,          # should hit cache if Step 2 already ran\n",
    "            request_delay_sec=0.6,\n",
    "        )\n",
    "\n",
    "    rows = parse_year_data_from_html(html, target_year=target_year)\n",
    "\n",
    "    print(f\"Parsed rows for appid={appid}, year={target_year}: {len(rows)}\")\n",
    "    print(\"Preview:\")\n",
    "    for r in rows[:12]:\n",
    "        print(r)\n",
    "\n",
    "# MAIN =======================================================================================================\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Central run configuration.\n",
    "\n",
    "    Keep all user-editable settings here so the rest of the code\n",
    "    stays stable and easy to reason about.\n",
    "    \"\"\"\n",
    "    # TODO: configure these\n",
    "    start_year = 2018\n",
    "    end_year = 2023\n",
    "    input_dir = Path(\".\")\n",
    "    input_pattern = \"{year}_top250_ids.csv\"\n",
    "    output_dir = Path(\"outputs\")\n",
    "    cache_dir = Path(\"steamcharts_cache\")\n",
    "\n",
    "    # TODO: call collect_year_range(...)\n",
    "    # TODO: print small preview (head) and summary counts\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _test_parse_html()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COGS108_FA25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
