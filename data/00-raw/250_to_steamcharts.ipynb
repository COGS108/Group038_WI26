{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53708734",
   "metadata": {},
   "source": [
    "# Collecting (month, average players, peak players) per game per month each year\n",
    "Takes in an array of arrays that looks like: [ [rank, app_id], [rank, app_id], ... ]\n",
    "Outputs: [ [app_id, month, avg_players, peak_players], ... ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e031862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and globals\n",
    "import csv\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path \n",
    "from typing import List, Tuple, Union, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # for parsing HTML docs\n",
    "\n",
    "# Base URL pattern for SteamCharts game pages\n",
    "# inject app_id into {app_id}, e.g. app_id=730 -> https://steamcharts.com/app/730\n",
    "BASE_URL = \"https://steamcharts.com/app/{app_id}\"\n",
    "\n",
    "# Apprently some sites block requests that do not provide a browser-like user agent.\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; SteamChartsYearScraper/1.0)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 250\n",
      "Columns: ['rank', 'name', 'appid']\n",
      " rank                  name   appid\n",
      "    1                  Rust  252490\n",
      "    2              Among Us  945360\n",
      "    3            The Forest  242760\n",
      "    4 Monster Hunter: World  582010\n",
      "    5                  DayZ  221100\n",
      "    6           Bloons TD 6  960090\n",
      "    7             Paladins®  444090\n",
      "    8            Subnautica  264710\n",
      "    9        Battlefield™ V 1238810\n",
      "   10              RimWorld  294100\n"
     ]
    }
   ],
   "source": [
    "# Some utlity helpers! ==========================================================================\n",
    "\n",
    "def clean_num(value: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Convert numeric text to float.\n",
    "\n",
    "    Examples:\n",
    "    - \"12,345.6\" -> 12345.6\n",
    "    - \"-\" or \"\"  -> None\n",
    "\n",
    "    Why this exists:\n",
    "    - SteamCharts numbers include commas and sometimes placeholders.\n",
    "    - Downstream analysis is easier if values are numeric.\n",
    "    \"\"\"\n",
    "    # TODO: strip whitespace, remove commas, handle blanks/dashes, return float or None\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def build_game_url(appid: int) -> str:\n",
    "    \"\"\"\n",
    "    Build SteamCharts URL for one appid.\n",
    "\n",
    "    Why this exists:\n",
    "    - Keeps URL format logic centralized in one place.\n",
    "    - Easier to change if site path ever changes.\n",
    "    \"\"\"\n",
    "    # TODO: return BASE_URL formatted with appid\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def cache_key_for_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Build deterministic cache filename key from URL.\n",
    "\n",
    "    Why this exists:\n",
    "    - URL text may not be ideal as a filename.\n",
    "    - Hash gives stable and filesystem-safe names.\n",
    "    \"\"\"\n",
    "    # TODO: return md5 hash hex string of the URL\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# Input and loading validation ==================================================================\n",
    "\n",
    "\n",
    "def load_input_csv(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and validate one input CSV.\n",
    "    (Probably not neccessary, but I think it's good practice just in case)\n",
    "\n",
    "    Required columns:\n",
    "    - rank\n",
    "    - name\n",
    "    - appid\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned DataFrame with normalized dtypes:\n",
    "      rank:int, name:str, appid:int\n",
    "\n",
    "    Why this exists:\n",
    "    - Fail fast with clear errors if schema is wrong.\n",
    "    - Avoid repeated type conversions elsewhere.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # Read CSV (consider encoding=\"utf-8-sig\")\n",
    "    df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Validate required columns\n",
    "    required = {\"rank\", \"name\", \"appid\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"{file_path.name} is missing required columns: {sorted(missing)}. \"\n",
    "            f\"Found columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Keep only needed columns in predictable order \n",
    "    df = df[[\"rank\", \"name\", \"appid\"]].copy()\n",
    "\n",
    "    # Convert numeric fields and fail LOUDLY if invalid\n",
    "    df[\"rank\"] = pd.to_numeric(df[\"rank\"], errors=\"raise\").astype(int)\n",
    "    df[\"appid\"] = pd.to_numeric(df[\"appid\"], errors=\"raise\").astype(int)\n",
    "\n",
    "    # 4) Strip whitespace on name\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.strip()\n",
    "\n",
    "    # Remove rows with empty names \n",
    "    df = df[df[\"name\"] != \"\"].copy()\n",
    "\n",
    "    # drop duplicates on rank+appid\n",
    "    df = df.drop_duplicates(subset=[\"rank\", \"appid\"]).reset_index(drop=True)\n",
    "\n",
    "    # Return cleaned DataFrame\n",
    "    # Sort by rank for deterministic processing\n",
    "    df = df.sort_values(\"rank\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Network + cache =================================================================================\n",
    "\n",
    "\n",
    "def get_game_page_html(\n",
    "    appid: int,\n",
    "    session: requests.Session,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return HTML for one game page, using cache when available.\n",
    "\n",
    "    Behavior:\n",
    "    - Build URL from appid\n",
    "    - If cached HTML exists and use_cache=True: return cached content\n",
    "    - Else fetch page from network, cache it, sleep briefly, return content\n",
    "\n",
    "    Why this exists:\n",
    "    - Separates network/caching from parsing logic.\n",
    "    - Prevents re-requesting same pages across reruns.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Ensure cache_dir exists\n",
    "    # 2) Compute URL and cache filename\n",
    "    # 3) If cache hit and use_cache: read + return\n",
    "    # 4) Else requests.get(...), raise_for_status()\n",
    "    # 5) Save HTML to cache\n",
    "    # 6) sleep(request_delay_sec)\n",
    "    # 7) return HTML\n",
    "    pass\n",
    "\n",
    "\n",
    "# HTML parsing ===================================================================================\n",
    "\n",
    "\n",
    "def parse_year_data_from_html(html: str, target_year: int) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parse SteamCharts monthly table from one app page, filtered to target year.\n",
    "\n",
    "    Input:\n",
    "    - html: raw page HTML\n",
    "    - target_year: year to keep (e.g., 2021)\n",
    "\n",
    "    Output row shape:\n",
    "    {\n",
    "      \"month\": \"YYYY-MM\",\n",
    "      \"avg_players\": float|None,\n",
    "      \"peak_players\": float|None\n",
    "    }\n",
    "\n",
    "    Why this exists:\n",
    "    - Pure parser function (HTML in -> structured rows out).\n",
    "    - Easy to test independently from I/O.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Parse html with BeautifulSoup\n",
    "    # 2) Select monthly table rows (table.common-table tbody tr)\n",
    "    # 3) For each row, parse month/avg/peak\n",
    "    # 4) Skip \"Last 30 Days\"\n",
    "    # 5) Parse month text with pd.to_datetime(..., format=\"%B %Y\")\n",
    "    # 6) Keep rows where parsed year == target_year\n",
    "    # 7) Clean numeric fields with clean_num()\n",
    "    # 8) Return rows sorted by month asc\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# year collection ====================================================================================\n",
    "\n",
    "\n",
    "def collect_one_year(\n",
    "    input_csv: Path,\n",
    "    year: int,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect SteamCharts data for one year's input list.\n",
    "\n",
    "    Steps:\n",
    "    - Load CSV (rank, name, appid)\n",
    "    - For each game:\n",
    "      - Fetch or read cached HTML\n",
    "      - Parse target-year monthly rows\n",
    "      - Emit result rows with status labels\n",
    "\n",
    "    Status values:\n",
    "    - \"ok\"               : parsed monthly rows exist\n",
    "    - \"no_data_for_year\" : page loaded, but no rows for that year\n",
    "    - \"request_error\"    : failed HTTP request\n",
    "    - \"parse_error\"      : page fetched but parse failed\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) games_df = load_input_csv(input_csv)\n",
    "    # 2) Initialize out_rows = []\n",
    "    # 3) Create requests.Session()\n",
    "    # 4) Loop games_df rows:\n",
    "    #    a) read rank, name, appid\n",
    "    #    b) try get_game_page_html(...)\n",
    "    #       - on failure append one request_error row, continue\n",
    "    #    c) try parse_year_data_from_html(...)\n",
    "    #       - on failure append one parse_error row, continue\n",
    "    #    d) if parsed rows empty:\n",
    "    #          append one no_data_for_year row\n",
    "    #       else:\n",
    "    #          append one row per month with status=\"ok\"\n",
    "    # 5) Build DataFrame with fixed column order\n",
    "    # 6) Return DataFrame\n",
    "    pass\n",
    "\n",
    "\n",
    "def collect_year_range(\n",
    "    start_year: int,\n",
    "    end_year: int,\n",
    "    input_dir: Path,\n",
    "    input_pattern: str,      # e.g. \"{year}_top250_ids.csv\"\n",
    "    output_dir: Path,\n",
    "    cache_dir: Path,\n",
    "    use_cache: bool = True,\n",
    "    request_delay_sec: float = 0.6,\n",
    "    write_combined: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run collection across a year range using predictable filenames.\n",
    "\n",
    "    Example:\n",
    "    - input_pattern \"{year}_top250_ids.csv\"\n",
    "    - for year=2018 -> \"2018_top250_ids.csv\"\n",
    "\n",
    "    Output:\n",
    "    - Writes one CSV per year to output_dir\n",
    "    - Optionally writes one combined CSV\n",
    "    - Returns combined DataFrame (or empty DataFrame if nothing processed)\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) Ensure output_dir exists\n",
    "    # 2) all_parts = []\n",
    "    # 3) for year in range(start_year, end_year + 1):\n",
    "    #    a) build input_csv path from pattern\n",
    "    #    b) if missing file: print skip and continue\n",
    "    #    c) year_df = collect_one_year(...)\n",
    "    #    d) write year_df to output_dir / f\"steamcharts_{year}_top250.csv\"\n",
    "    #    e) append year_df to all_parts\n",
    "    # 4) if all_parts empty: return empty DataFrame with expected columns\n",
    "    # 5) combined_df = concat all_parts\n",
    "    # 6) if write_combined: save combined CSV\n",
    "    # 7) return combined_df\n",
    "    pass\n",
    "\n",
    "# TESTING ===================================================================================================\n",
    "def _test_load_input_csv():\n",
    "    test_path = Path(\"2018_top250_ids.csv\")\n",
    "    df = load_input_csv(test_path)\n",
    "\n",
    "    print(\"Loaded rows:\", len(df))\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(df.head(10).to_string(index=False))\n",
    "\n",
    "# MAIN =======================================================================================================\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Central run configuration.\n",
    "\n",
    "    Keep all user-editable settings here so the rest of the code\n",
    "    stays stable and easy to reason about.\n",
    "    \"\"\"\n",
    "    # TODO: configure these\n",
    "    start_year = 2018\n",
    "    end_year = 2023\n",
    "    input_dir = Path(\".\")\n",
    "    input_pattern = \"{year}_top250_ids.csv\"\n",
    "    output_dir = Path(\"outputs\")\n",
    "    cache_dir = Path(\"steamcharts_cache\")\n",
    "\n",
    "    # TODO: call collect_year_range(...)\n",
    "    # TODO: print small preview (head) and summary counts\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _test_load_input_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COGS108_FA25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
